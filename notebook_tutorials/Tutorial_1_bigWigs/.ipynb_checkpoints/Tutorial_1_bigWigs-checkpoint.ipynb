{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f298f4e-e9f4-4181-8945-35f7e8682734",
   "metadata": {
    "id": "9f298f4e-e9f4-4181-8945-35f7e8682734"
   },
   "source": [
    "# **Tutorial 1. Dealing with bigWig Data**\n",
    "\n",
    "BigWig files are compressed binary files developed by UCSC. They are primarily used in bioinformatics to store and visualise genomic data in UCSC's Genome Browser. The files are indexed allowing access to specific genomic regions without having to access or download the whole genome. Accessing and converting their data for machine learning purposes can be quite confusing at times. UCSC has their own programs to extract genomic data from bigWigs, however python libraries like pyBigWig also exist, making converting bigWigs to numpy arrays much easier.\n",
    "\n",
    "This tutorial begins using UCSC's programs to quickly understand the genomic data within BigWigs, before using the pyBigWig library to simply extract BigWig data.\n",
    "\n",
    "The final part of this tutorial uses the pyBigWig library to load, filter, and split BigWig data into training, validation, and test sets. The data consists of signal p-values from ChIP-seq experiments, processed using the MACS2 tool, which outputs signals averaged over 25 base pair bins. We will re-average these signals to a resolution of 32 base pairs (changes depending on model requirements). Additionally, we will implement threshold-based filtering and consistent data splits to ready the data for a model.\n",
    "\n",
    "This tutorial uses ChIP-seq histone data from two experiments from the Encode project with accession ids: <br>\n",
    "[ENCSR817LUF](https://www.encodeproject.org/experiments/ENCSR817LUF/) <br>\n",
    "[ENCSR958TSO](https://www.encodeproject.org/experiments/ENCSR958TSO/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4oxNkQY-jxK1",
   "metadata": {
    "id": "4oxNkQY-jxK1"
   },
   "outputs": [],
   "source": [
    "!pip install numpy --quiet\n",
    "!pip install pyBigWig --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d436c-dd4c-4fa4-9552-df51e024959c",
   "metadata": {
    "id": "0d1d436c-dd4c-4fa4-9552-df51e024959c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyBigWig\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91826275-f54f-4003-9f3e-a156bee097eb",
   "metadata": {
    "id": "91826275-f54f-4003-9f3e-a156bee097eb"
   },
   "source": [
    "## **UCSC Programs**\n",
    "\n",
    "In order to get a quick sense of the data in a bigWig file, lets use UCSC's programs. <br><br>\n",
    "UCSC's programs to access information from bigWigs include:\n",
    "\n",
    "**bigWigInfo** — prints out information about a bigWig file. <br><br>\n",
    "**bigWigSummary** — extracts summary information from a bigWig file. <br><br>\n",
    "\n",
    "Additional UCSC programs include: <br> <br>\n",
    "- bigWigToWig  <br><br>\n",
    "- bigWigToBedGraph <br><br>\n",
    "These programs allow for converting bigWig files into other formats such as .wig and .bedgraph if needed. <br><br>\n",
    "\n",
    "Available to download from \"https://genome.ucsc.edu/goldenPath/help/bigWig.html\". The executable programs are within this tutorial's github repository.\n",
    "\n",
    "Running the programs by themselves print out their help/usage statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wrx2-4Wbj4vr",
   "metadata": {
    "id": "Wrx2-4Wbj4vr"
   },
   "outputs": [],
   "source": [
    "! wget https://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/bigWigInfo \"bigWigInfo\"\n",
    "! chmod +x bigWigInfo\n",
    "\n",
    "! wget https://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/bigWigSummary \"bigWigSummary\"\n",
    "! chmod +x bigWigSummary\n",
    "\n",
    "# Change linux to macOSX.x86_64 if running locally on a mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b244f-b8d5-4681-a2c8-f3e5d3064ad7",
   "metadata": {
    "id": "959b244f-b8d5-4681-a2c8-f3e5d3064ad7"
   },
   "outputs": [],
   "source": [
    "!./bigWigInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "azfwRV34qUsR",
   "metadata": {
    "id": "azfwRV34qUsR"
   },
   "outputs": [],
   "source": [
    "!./bigWigSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e8445-08ab-490f-80b8-34bf32a76e01",
   "metadata": {
    "id": "592e8445-08ab-490f-80b8-34bf32a76e01"
   },
   "source": [
    "## bigWigInfo\n",
    "**BigWig files (.bw) can be accessed via url**, however it is recommended that data be downloaded locally. If you look at genomic data from the [Encode Project](https://www.encodeproject.org), processed data such as p-values and fold-change data are in bigWig format. The bigWig file is available to be downloaded or accessed from \"https://www.encodeproject.org/files/[experiment]/@@download/[experiment].bigWig\", where [experiment] is the accession id of the experiment. We can parse the indexed data without having to download the files themselves.\n",
    "\n",
    "Take the accession id [ENCSR817LUF](https://www.encodeproject.org/experiments/ENCSR817LUF/). This is the same experiment that we visualised earlier using the UCSC Genome Browser (1.2.2 Example Data Representations:). Recall it was a ChIP-seq experiment targeting the H3K36me3 histone modification in brain tissue.\n",
    "\n",
    "The bigWig file containing the p-value for signal strength for this experiment has the accession id [ENCFF601VTB](https://www.encodeproject.org/files/ENCFF601VTB/). The specific accession id's for bigWig files can be found in the \"Association graph\" or \"File details\" tab from any experiment ([ENCSR817LUF](https://www.encodeproject.org/experiments/ENCSR817LUF/)).\n",
    "<br> <br>Lets run UCSC's programs on this bigWig.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf74e2-3a15-45e8-a519-7358e46cc893",
   "metadata": {
    "id": "c9cf74e2-3a15-45e8-a519-7358e46cc893"
   },
   "outputs": [],
   "source": [
    "!./bigWigInfo https://www.encodeproject.org/files/ENCFF601VTB/@@download/ENCFF601VTB.bigWig\n",
    "\n",
    "#!./bigWigInfo https://www.encodeproject.org/files/ENCFF601VTB/@@download/ENCFF601VTB.bigWig -chroms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3b41c-5c28-4324-a609-7128475137b0",
   "metadata": {
    "id": "e1d3b41c-5c28-4324-a609-7128475137b0"
   },
   "source": [
    "## bigWigSummary\n",
    "\n",
    "bigWigSummary allows us to view summary statistics (-type=mean/min/max/std/coverage) from specified genomic regions within chromosomes.\n",
    "This is done using \"bigWigSummary -type=mean/min/max/std/coverage file.bigWig chrom start end dataPoints\". The start and end positions represent the genomic coordinates we are interested in, while dataPoints are simply the number of bins we want to summarise the data into.\n",
    "\n",
    "Earlier using the UCSC Genome Browser (1.2.2 Example Data Representations:) we visualised the p-value signals from chromosome 1, within the region: 11,084,744 - 11,095,920. Lets summarise the mean of p-values across this genomic region into 10 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9ac33-ce09-4d2a-bf0e-8b255a35cecc",
   "metadata": {
    "id": "0de9ac33-ce09-4d2a-bf0e-8b255a35cecc"
   },
   "outputs": [],
   "source": [
    "!./bigWigSummary -type=mean https://www.encodeproject.org/files/ENCFF601VTB/@@download/ENCFF601VTB.bigWig chr1 11084744 11095920 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6839db-425a-4bf6-aaa5-27daa4044175",
   "metadata": {
    "id": "6a6839db-425a-4bf6-aaa5-27daa4044175"
   },
   "source": [
    "<br>**What do these bins represent?** <br><br>\n",
    "In essence, we've taken a genomic region (11,084,744 - 11,095,920) of 11,176 base pairs long, and averaged p-values over 10 bins of (11,176/10) 1,117.6 base pairs long resulting in 10 averaged p-values across the region. <br>\n",
    "\n",
    "When training a machine learning model, we need to decide what prediction resolution in base pairs we want to look at and what the aim of the model is. One example would be to take an input window e.g. 2048bps, and aim to predict the average p-value (coverage) within a smaller resolution e.g. 32bps at the center of the input window. Essentially, each chromosome for each bigWig (from multiple experiments) would be broken up into 2048bp bins, where the center 32bps are averaged. The coverage for each bin would be used as a target value (Y) for each 2048bp region. Given that the experiment data we're looking at targets the H3K36me3 histone modeification, we could aim to understand and predict this modification patterns across cell types.\n",
    "\n",
    "\n",
    "<br><br>![Alt Text](https://nbviewer.org/github/sn2023imperial/tutorial_1/blob/main/bp_resolution.png)\n",
    "<br>[Jai Patel. Leveraging Genetic Diversity With\n",
    "Machine Learning. MENG INDIVIDUAL PROJECT, Imperial College London.]\n",
    "\n",
    "<br> This image shows visually the 32bp resolution that we are aiming for. Note that the sizes of the input window (2048bp), output window (32bp) and prediction resolution (32bp) are variables that we decide to set depending on our model requirements. Changing these windows affects the regions our model learns from and predicts.<br>\n",
    "\n",
    "Using this example, the cell below shows what the first 2048bp input window would look like. So for training our imagined model, the X values are the 2048bps region 0 - 2048 (later one-hot-encoded as DNA) and the target value (Y) is the 32bp (output window) averaged p-values in the center of the 2048bp region = 0.03324. Of course, however, we would need to shift this input window accross across chromosomes and potentially for multiple experiments to complete the dataset.\n",
    "\n",
    "\n",
    "It could also be the case that you choose to predict for the centre X base-pairs at the averaged level. For example, you could choose to predict the centre (32*6) 192 base-pairs, i.e. 6 values max coverage values. This depends on the model design and aim. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f12a93-e391-41ed-b6e3-03fb719dc0a3",
   "metadata": {
    "id": "24f12a93-e391-41ed-b6e3-03fb719dc0a3"
   },
   "outputs": [],
   "source": [
    "!./bigWigSummary -type=mean https://www.encodeproject.org/files/ENCFF601VTB/@@download/ENCFF601VTB.bigWig chr1 1008 1040 1\n",
    "\n",
    "# The prediction region would be the center of the input window 0 to 2048. e.g. from 1008 to 1040. These 32 p-values are averaged.\n",
    "# The next input window would be from 2048 to 4096, where again the center 32bps will be averaged to create a target value (Y).\n",
    "\n",
    "#e.g.\n",
    "#!./bigWigSummary -type=mean https://www.encodeproject.org/files/ENCFF601VTB/@@download/ENCFF601VTB.bigWig chr1 3056 3088 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b9657-bee5-413d-890d-44115aab5e77",
   "metadata": {
    "id": "5e1b9657-bee5-413d-890d-44115aab5e77"
   },
   "source": [
    "## USING pyBigWig\n",
    "\n",
    "https://github.com/deeptools/pyBigWig?tab=readme-ov-file#load-the-extension\n",
    "\n",
    "The pyBigWig python library allows for an easier experience with accessing and manipulating bigWig files than UCSC's programs. Some of the library's functions include the following, however, for the most part you would only be using '.open' and '.values' within your script.\n",
    "\n",
    "bw = **pyBigWig.open** <br>\n",
    "bw **.header**  (Similar to UCSC's bigWigInfo)<br>\n",
    "bw **.stats** (Similar to UCSC's bigWigSummary)<br>\n",
    "bw **.values** <br>\n",
    "bw **.intervals** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JWwEPBUomz56",
   "metadata": {
    "id": "JWwEPBUomz56"
   },
   "outputs": [],
   "source": [
    "# Downloading the bigWigs locally\n",
    "! wget https://www.encodeproject.org/files/ENCFF601VTB/@@download/ENCFF601VTB.bigWig \"ENCFF601VTB.bigWig\"\n",
    "! wget https://www.encodeproject.org/files/ENCFF042SAY/@@download/ENCFF042SAY.bigWig \"ENCFF042SAY.bigWig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485faabd-1ce1-4f08-bda5-acb54c51a146",
   "metadata": {
    "id": "485faabd-1ce1-4f08-bda5-acb54c51a146"
   },
   "outputs": [],
   "source": [
    "bw = pyBigWig.open(\"ENCFF601VTB.bigWig\")\n",
    "\n",
    "# Specify values for a specific region\n",
    "chrom = \"chr1\"\n",
    "start = 11084744\n",
    "end = 11095920     # We'll use the same region and bigWig (ENCFF601VTB) we were exploring above\n",
    "\n",
    "# Get values for the specified interval\n",
    "values = bw.values(chrom, start, end, numpy=True) # List containing one value for every base pair, converted to numpy array\n",
    "\n",
    "# Using .intervals\n",
    "intervals = bw.intervals(chrom, start, end)\n",
    "intervals = np.array(intervals)\n",
    "\n",
    "# Header\n",
    "header = bw.header()\n",
    "\n",
    "# Stats\n",
    "mean = bw.stats(\"chr1\", start, end, type=\"mean\")\n",
    "exactmean = bw.stats(\"chr1\", start, end, type=\"mean\", exact=True)\n",
    "\n",
    "# Close the BigWig file\n",
    "bw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fcc35c-70bb-4b0f-bbf2-a74b5cef1a4d",
   "metadata": {
    "id": "21fcc35c-70bb-4b0f-bbf2-a74b5cef1a4d"
   },
   "outputs": [],
   "source": [
    "print(header)\n",
    "\n",
    "# bw.header gives information on the bigWig as a whole, similar to UCSC's bigwiginfo program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e771b9-2b22-453d-ad51-a84c63735545",
   "metadata": {
    "id": "62e771b9-2b22-453d-ad51-a84c63735545"
   },
   "outputs": [],
   "source": [
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058abdf1-b243-4072-a2d5-fb06dd17470b",
   "metadata": {
    "id": "058abdf1-b243-4072-a2d5-fb06dd17470b"
   },
   "outputs": [],
   "source": [
    "print(exactmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d31af8-01ca-4eaa-8112-acbc9f83d2fe",
   "metadata": {
    "id": "e6d31af8-01ca-4eaa-8112-acbc9f83d2fe"
   },
   "outputs": [],
   "source": [
    "print(np.mean(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73209a55-bffa-464f-abbd-21efc6896a71",
   "metadata": {
    "id": "73209a55-bffa-464f-abbd-21efc6896a71"
   },
   "source": [
    "### Summary Statistics Note\n",
    "\n",
    "The discrepancy between stats and statstrue is due to the way in which data is accessed in bigWig files. As they were created for genome browsers, the speed of visualising large amounts of data is important versus exact statistics. Summary statistics are affected by the zoom levels or 'nlevels'. When using numpy or the 'exact=True' tag to calculate the mean, the results are exact.\n",
    "\n",
    "\"When a bigWig file is queried for a summary statistic, the size of the interval is used to determine whether to use a zoom level and, if so, which one. The optimal zoom level is that which has the largest bins no more than half the width of the desired interval. If no such zoom level exists, the original intervals are instead used for the calculation.\" [pybigwig](https://github.com/deeptools/pyBigWig?tab=readme-ov-file#a-note-on-statistics-and-zoom-levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cad73a-2b77-45ee-a0c6-842809ee7136",
   "metadata": {
    "id": "e4cad73a-2b77-45ee-a0c6-842809ee7136"
   },
   "source": [
    "### Comparing Values vs Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0d197-8bb7-4ebb-b72d-2dfa7efc50c9",
   "metadata": {
    "id": "10a0d197-8bb7-4ebb-b72d-2dfa7efc50c9"
   },
   "outputs": [],
   "source": [
    "print(\"Values Data with shape:\", values.shape, \"\\n\")\n",
    "print(values[0:5])\n",
    "\n",
    "\n",
    "print(\"\\n\\nIntervals Data with shape:\", intervals.shape, \"\\n\")\n",
    "# Print the start position, end position, and value for each interval\n",
    "for interval in intervals[0:5]:\n",
    "    print(f\"Start: {interval[0]}, End: {interval[1]}, Value: {interval[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b38d4a-6632-49d4-af6a-d3d1180a5f1d",
   "metadata": {
    "id": "90b38d4a-6632-49d4-af6a-d3d1180a5f1d"
   },
   "source": [
    "<br>\n",
    "\n",
    "The **'Intervals Data'** represents p-values, where the p-values are constant within each defined interval. For machine learning purposes while variable-length intervals can be used to train models, it requires a higher level of data handling (potential issues due to imbalanced classes) and may increase the computational complexity. <br>\n",
    "\n",
    "The **'Values Data'** represents every data point in the selected genomic region. This data allows for more flexibility in the data preprocessing stage, where we can set the input window, target window and prediction resolution to suit our model. Each data point represents 1 base pair. However, because peak calling was done using the MACS2 tool, these values have already been averaged at 25 base-pairs to reduce noise. Averaging below 25bps would create data integrity issues. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73eda02-2b21-45e0-96d7-25a540d378bd",
   "metadata": {
    "id": "e73eda02-2b21-45e0-96d7-25a540d378bd"
   },
   "outputs": [],
   "source": [
    "# Plotting the p-value data\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(range(start, end), values, color='blue')\n",
    "plt.fill_between(range(start, end), values, color='blue', alpha=0.3)\n",
    "plt.title(f\"Read Coverage in {chrom}:{start}-{end}\")\n",
    "plt.xlabel(\"Genomic Position\")\n",
    "plt.ylabel(\"Coverage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eadf539-1564-45c0-9aa3-e73b6b2b5f6e",
   "metadata": {
    "id": "2eadf539-1564-45c0-9aa3-e73b6b2b5f6e"
   },
   "source": [
    "![Alt text](https://nbviewer.org/github/sn2023imperial/tutorial_1/blob/main/ENCFF601VTB_genome_browser.png) <br>\n",
    "\n",
    "Run the code above to see the values extracted from the ENCFF601VTB bigWig compared to the same region as seen in [UCSC's Genome Browser](https://genome.ucsc.edu/cgi-bin/hgTracks?db=hg38&lastVirtModeType=default&lastVirtModeExtraState=&virtModeType=default&virtMode=0&nonVirtPosition=&position=chr1%3A11084744%2D11095920&hgsid=2307713234_Kap236Tjt6ZGnnNrXMkIhq2Ajn27). As you can see, the peak regions match and we can continue processing the p-values for a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45d611d-a168-44dd-a291-3fdad1a72eaa",
   "metadata": {
    "id": "c45d611d-a168-44dd-a291-3fdad1a72eaa"
   },
   "source": [
    "# Example Data Pre-Processing\n",
    "\n",
    "## Part 1 - Target Values (Y)\n",
    "\n",
    "Lets say we wanted to predict the coverage from this same experiment ENCSR817LUF, and another experiment for the sake of processing data from multiple experiments; ENCSR958TSO, which also targets the H3K36me3 histone. How would the data pre-processing stage look in practice? First we would need to decide our model constants, (e.g. which chromosomes, the input window, output window and prediction resolution), have an array containing chromosome lengths for bin buffer lengths, and bigWig data. For computational reasons, lets go through the process for chromosomes 1 through 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b22512-63be-4749-a791-5c0a31b9490c",
   "metadata": {
    "id": "76b22512-63be-4749-a791-5c0a31b9490c"
   },
   "outputs": [],
   "source": [
    "# Model constants\n",
    "INPUT_WINDOW = 2048\n",
    "OUTPUT_WINDOW = 32\n",
    "PRED_RES = 32\n",
    "buffer_bp = (INPUT_WINDOW-OUTPUT_WINDOW)//2\n",
    "\n",
    "\n",
    "# Model will predict on chromsomes 1\n",
    "CHROMOSOMES =np.array(['chr1', 'chr2', 'chr3'])\n",
    "\n",
    "\n",
    "# hg38 chrom lengths\n",
    "# Human Genome Assembly GRCh38.p14\n",
    "CHROM_LEN =np.array([248_956_422, 242_193_529, 198_295_559, 190_214_555, 181_538_259, 170_805_979,\n",
    "                     159_345_973, 145_138_636, 138_394_717, 133_797_422, 135_086_622, 133_275_309,\n",
    "                     114_364_328, 107_043_718, 101_991_189, 90_338_345, 83_257_441, 80_373_285,\n",
    "                     58_617_616, 64_444_167, 46_709_983, 50_818_468])\n",
    "\n",
    "\n",
    "# URLs of the BigWig files\n",
    "urls = [\n",
    "    'ENCFF601VTB.bigWig',\n",
    "    'ENCFF042SAY.bigWig' # p-values from experiment ENCSR958TSO\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a42faed-2558-43ef-b396-1dc4cbd07a40",
   "metadata": {
    "id": "6a42faed-2558-43ef-b396-1dc4cbd07a40"
   },
   "source": [
    "### Function to Process Target Values (Y)\n",
    "\n",
    "As explained previously, the aim for one snapshot of the input window as it goes over a chromosome, is to average a smaller resolution of base pairs across the center of the input window. This load_y function implements this logic. An arcsinh transformation is included to help deal normalise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cfd946-efd1-4064-8119-2920a8c4f2cd",
   "metadata": {
    "id": "b2cfd946-efd1-4064-8119-2920a8c4f2cd"
   },
   "outputs": [],
   "source": [
    "def load_y(data: dict,#pybigwig object\n",
    "           labels: list,\n",
    "           selected_chromosome: chr,\n",
    "           window_start: int,\n",
    "           window_size: int,\n",
    "           target_bp:int,\n",
    "           pred_res: int):\n",
    "\n",
    "    \"\"\"Function to load target y values from bigwigs\"\"\"\n",
    "    #get buffer amount between input and output - centre output\n",
    "    buffer_bp = (window_size-target_bp)//2\n",
    "    target_length = target_bp//pred_res\n",
    "    # Output labels only for selected cells\n",
    "    all_y = np.zeros(shape=(target_length, len(labels)))\n",
    "    for i, label in enumerate(labels):\n",
    "        #data at pred_res bp lvl already but loaded in at 1bp lvl\n",
    "        #need to avg back up!\n",
    "        # Data is arcsinh transformed to deal help with different seq depths\n",
    "        all_y[:, i] = np.arcsinh(np.mean(\n",
    "            np.nan_to_num(\n",
    "                data[label].values(\n",
    "                    selected_chromosome,\n",
    "                    window_start+buffer_bp,\n",
    "                    window_start+buffer_bp+target_bp,\n",
    "                    numpy=True\n",
    "                )\n",
    "            ).reshape(-1, pred_res),#averaging at desired pred_res\n",
    "            axis=1))\n",
    "    return all_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aaf0f4-6cf8-411c-8fb1-946e1fb42699",
   "metadata": {
    "id": "36aaf0f4-6cf8-411c-8fb1-946e1fb42699"
   },
   "source": [
    "### Main Script\n",
    "\n",
    "The 2nd main portion of this code takes a long time to execute eventhough we're going through just three chromosome from two experiments. The data output is saved as a compressed csv file. Running Main 1 will give you a sense of what the dna bins look like. In terms of code logic, we create the bins based on the input window size (2048), starting from the buffer (1008) and ending one buffer before the end of a chromosome in order to prevent going past the chromosome indexes. Then we populate the dataframe using our load_y function which transforms our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19efb5a9-1031-4dad-ad8b-27ec3eb9590c",
   "metadata": {
    "id": "19efb5a9-1031-4dad-ad8b-27ec3eb9590c"
   },
   "outputs": [],
   "source": [
    "# Main 1\n",
    "\n",
    "# Create a dict of all bigwigs\n",
    "bw_dat = {}\n",
    "for url in urls:\n",
    "    bw = pyBigWig.open(url)\n",
    "    if bw is not None:\n",
    "        bw_dat[url] = bw\n",
    "\n",
    "\n",
    "\n",
    "# Setting up the dna bins and dataframe\n",
    "for ind,chrom_i in enumerate(CHROMOSOMES):\n",
    "    bins = np.arange(buffer_bp,CHROM_LEN[ind]-buffer_bp,INPUT_WINDOW)\n",
    "    #store in pd df\n",
    "    if(ind==0):\n",
    "        dna_bins = pd.DataFrame({\n",
    "            'chr': chrom_i,\n",
    "            'pred_strt': bins,\n",
    "            'pred_end': bins+OUTPUT_WINDOW,\n",
    "            'dna_strt': bins-buffer_bp,\n",
    "            'dna_end': bins+OUTPUT_WINDOW+buffer_bp\n",
    "        })\n",
    "    else:\n",
    "        tmp = pd.DataFrame({\n",
    "            'chr': chrom_i,\n",
    "            'pred_strt': bins,\n",
    "            'pred_end': bins+OUTPUT_WINDOW,\n",
    "            'dna_strt': bins-buffer_bp,\n",
    "            'dna_end': bins+OUTPUT_WINDOW+buffer_bp\n",
    "        })\n",
    "        dna_bins = pd.concat([dna_bins, tmp])\n",
    "dna_bins = dna_bins.reset_index()\n",
    "print(dna_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad5fe1-37fd-447c-9044-ef42053e0b71",
   "metadata": {
    "id": "c7ad5fe1-37fd-447c-9044-ef42053e0b71"
   },
   "outputs": [],
   "source": [
    "# Main 2 WARNING THIS CODE BLOCK TAKES A LONG TIME TO RUN.\n",
    "\n",
    "\n",
    "# For each bin, load the data and find the max coverage\n",
    "cov = []\n",
    "for index, row in dna_bins.iterrows():\n",
    "    if(index%20_000==0):\n",
    "        print(row['chr'] +\": \"+str(row['pred_strt']))\n",
    "    dna_strt = row['dna_strt']\n",
    "    the_chr = row['chr']\n",
    "    all_y = load_y(data=bw_dat,\n",
    "                    labels=urls,\n",
    "                    selected_chromosome=the_chr,\n",
    "                    window_start=dna_strt,\n",
    "                    window_size = INPUT_WINDOW,\n",
    "                    target_bp = OUTPUT_WINDOW,\n",
    "                    pred_res = PRED_RES)\n",
    "    #save max coverage\n",
    "    cov.append(np.max(all_y))\n",
    "\n",
    "#add cov to data\n",
    "dna_bins['cov'] = cov\n",
    "\n",
    "\n",
    "\n",
    "# Save the data as a compressed csv file\n",
    "\n",
    "dna_bins.to_csv('reg_cov_p.csv.gz',\n",
    "                index=False,compression='gzip')\n",
    "\n",
    "\n",
    "print(dna_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9546a38-272b-4fae-a1ae-def9cd5de448",
   "metadata": {
    "id": "a9546a38-272b-4fae-a1ae-def9cd5de448"
   },
   "source": [
    "## Part 2 - X Values, Thresholding, One-Hot-Encoding, and Train/Test Splits\n",
    "\n",
    "Given our input window size of 2048bp, we have averaged the center 32bps to give us the max coverage, our target values (y). Now we need to collect our X values (DNA) to train our model to predict the target values. As stated previously, the X values are the 2048bp region in the chromosome, e.g. dna_strt - dna_end. Because these ENCODE bigWigs have already been alligned to a reference genome, we can retrieve these 2048bp regions as DNA sequences (A, C, T, and Gs,) using the reference genome hg38. For our model, we'll one-hot-encode these sequences, and split the data into training, validation and test sets saved as npz files for ease of loading the data into a model later.\n",
    "\n",
    "The reference genome data is located in this repo (data/hg38_reference_genome/hg38.fa) but can be downlaoded from the following UCSC link <br>(http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz) using \"wget -O\" on the command line <br>\n",
    "\n",
    "wget -O - http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c > hg38.fa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72290c-a338-4b53-8f0c-6244eff6b864",
   "metadata": {
    "id": "7d72290c-a338-4b53-8f0c-6244eff6b864"
   },
   "source": [
    "### Thresholding\n",
    "\n",
    "We'll begin by loading our dna_bins from the previous part and undergo thresholding. The threshold value of 2 is chosen in consistency with [a previous genomic study](https://www.nature.com/articles/s42256-022-00570-9) on evaluating deep learning for predicting epigenomic profiles. We aim to only select bins with max coverage values over this threshold to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03329a-d4c2-4ce3-8ad7-9db1850ed643",
   "metadata": {
    "id": "5c03329a-d4c2-4ce3-8ad7-9db1850ed643"
   },
   "outputs": [],
   "source": [
    "# Load our DNA bins filled with our target values\n",
    "dna_bins = pd.read_csv('reg_cov_p.csv.gz')\n",
    "\n",
    "# Set a threshold to filter our data\n",
    "threshold = 2\n",
    "threshold = np.arcsinh(threshold) # The threshold has to undergo an arcsinh transformation as well\n",
    "\n",
    "filt_dna_bins = dna_bins.loc[dna_bins['cov']>threshold].reset_index(drop=True) # Apply the threshold\n",
    "\n",
    "print(f\"{filt_dna_bins.shape[0]} training/validation/test positions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b951a47-d37c-4d42-b46b-6a990b3e6c4e",
   "metadata": {
    "id": "4b951a47-d37c-4d42-b46b-6a990b3e6c4e"
   },
   "source": [
    "### Function for One-Hot-Encoding\n",
    "\n",
    "The aim of this function is to one-hot-encode a sequence of A, C, T, and Gs from the reference genome hg38. The function returns a numpy array of shape (sequence_length, 4). Note that the code handles 'N' as a special case and sets all bases for that nucelotide to False/0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fdd371-c264-40c1-989a-45a8b4e57016",
   "metadata": {
    "id": "09fdd371-c264-40c1-989a-45a8b4e57016"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_sequence(sequence: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to one-hot encode a given DNA sequence.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (str): The DNA sequence consisting of nucleotides 'A', 'C', 'G', 'T', 'N'\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert the sequence to uppercase\n",
    "    sequence = sequence.upper()\n",
    "\n",
    "    # Define nucleotide mapping\n",
    "    nucleotide_mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "    # Initialize the one-hot encoded matrix with zeros\n",
    "    one_hot_matrix = np.zeros((len(sequence), len(nucleotide_mapping)), dtype=int)\n",
    "\n",
    "    for i, nucleotide in enumerate(sequence):\n",
    "        if nucleotide in nucleotide_mapping:\n",
    "            one_hot_matrix[i, nucleotide_mapping[nucleotide]] = 1\n",
    "        elif nucleotide == 'N':\n",
    "            # 'N' case: all zeros\n",
    "            continue\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid nucleotide '{nucleotide}' in the sequence.\")\n",
    "\n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f516c6a-03f6-4b94-80ae-70f8cca22001",
   "metadata": {
    "id": "5f516c6a-03f6-4b94-80ae-70f8cca22001"
   },
   "source": [
    "### Training/Validation/Test Splits\n",
    "\n",
    "In order to prepare our data for a machine learning model, we need training, validation and test splits. The key point of the function is that we are splitting by chromosome. Splitting by chromosomes ensures that regions from the same chromosome are not split across training, validation, and test sets. This preserves the genomic context and avoids data leakage where information from the same chromosome might inadvertently influence multiple splits. If genomic regions from the same chromosome are in both training and test sets, it could lead to overfitting or biased evaluation, and this ensures each split is independent. Several pitfalls of machine learning in genomics will be discussed in further tutorials.\n",
    "\n",
    "The function first selects a set of chromosomes to include in the test set based on the desired coverage fraction. Once a chromosome is selected for the test set, it is removed from the list of available chromosomes for subsequent sets. This is then repeated for the validation set, and the remaining chromosomes are used for the training set. As we are only using 3 chromosomes, I have increased the tolerance for the splits to allow for the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd15fc7-6713-4781-9338-0d3c80a955e0",
   "metadata": {
    "id": "5bd15fc7-6713-4781-9338-0d3c80a955e0"
   },
   "outputs": [],
   "source": [
    "def train_valid_test_split(dat, valid_frac=0.2, test_frac=0.2,tol=0.001):\n",
    "    \"\"\"\n",
    "    Function to create a train validatin test split between chromosomes.\n",
    "    Takes all training regions and splits to give proportions based on\n",
    "    user selected.\n",
    "    \"\"\"\n",
    "    #get the coverage filtered dataset to group by chrom and get num entries\n",
    "    counts_chroms = dat.groupby('chr')['cov'].count().reset_index()\n",
    "    counts_chroms['prop'] = counts_chroms['cov']/counts_chroms['cov'].sum()\n",
    "\n",
    "    def sample_set(counts_chroms,frac = 0.2,tol=0.025):\n",
    "        #make a copy so can reset\n",
    "        counts_chroms_ = counts_chroms.copy()\n",
    "        reset_count=0\n",
    "        act_frac=0\n",
    "        sample_chr=[]\n",
    "        while abs(act_frac-frac)>tol:\n",
    "            #if gone to high, start again\n",
    "            if act_frac>frac:\n",
    "                sample_chr=[]\n",
    "                act_frac=0\n",
    "                counts_chroms_ = counts_chroms.copy()\n",
    "                reset_count+=1\n",
    "                assert reset_count<100, f\"Can't find a combination for {frac} proportion!\"\n",
    "            else:\n",
    "                #sample rand chrom\n",
    "                chr_rand = random.randrange(counts_chroms_.shape[0])\n",
    "                sample_chr.append(counts_chroms_.loc[chr_rand]['chr'])\n",
    "                act_frac+=counts_chroms_.loc[chr_rand]['prop']\n",
    "                counts_chroms_ = counts_chroms_.drop(index=[chr_rand]).reset_index(drop=True)\n",
    "        return(counts_chroms_,sample_chr,act_frac)\n",
    "    #get test set\n",
    "    counts_chroms,test_chr,test_act = sample_set(counts_chroms,frac = test_frac,tol=tol)\n",
    "    #get valid set\n",
    "    counts_chroms,valid_chr,valid_act = sample_set(counts_chroms,frac = valid_frac,tol=tol)\n",
    "    #train - remainder\n",
    "    train_chr = counts_chroms['chr'].tolist()\n",
    "    train_act = counts_chroms['prop'].sum()\n",
    "\n",
    "    return({\"train\":train_chr,\"valid\":valid_chr,\"test\":test_chr},\n",
    "           {\"train\":train_act,\"valid\":valid_act,\"test\":test_act}\n",
    "           )\n",
    "\n",
    "chrom_split,props = train_valid_test_split(filt_dna_bins, valid_frac=0.2, test_frac=0.2,tol=0.14)\n",
    "print(chrom_split)\n",
    "print(props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ef0a7-1fea-4776-9c68-b7ba94d53867",
   "metadata": {
    "id": "bb7ef0a7-1fea-4776-9c68-b7ba94d53867"
   },
   "source": [
    "### Putting It All Together Into .npz Files\n",
    "\n",
    "Due to computational limitations, the reference genome hg38 is accessed using Ensembl's REST API. However it is recommended that all data, including the reference genome is downloaded locally. When downloaded locally, saving the reference genome as a variable is simple using the .Fastafile(path) method from the Pysam library. Accessing a region is as simple using Pysam's .fetch(chr, start, end) method.\n",
    "\n",
    "Following on, we'll use a random seed for reproducibility of the splits. We'll call our train/valid/test function to split our filtered dna bins, adding the split as a column to the data. Initialising a counters dictionary to keep track, for each row in the data (representing an input window size region), we'll call our one-hot-encode function to encode the 2048bp region. Finally, within this for loop, we'll save each file as an .npz file by their split and counter.\n",
    "\n",
    "For this example, I have chosen to break the loop after one .npz split is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526c473-4b26-438b-b332-50126aef0a85",
   "metadata": {
    "id": "f526c473-4b26-438b-b332-50126aef0a85"
   },
   "outputs": [],
   "source": [
    "def genome_dat(chromosome, start, end):\n",
    "    start = start + 1 # Not including start bp, Pysam.fetch by default does not include the start bp.\n",
    "\n",
    "    url = f'https://rest.ensembl.org/sequence/region/human/{chromosome}:{start}..{end}?content-type=text/x-fasta'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.ok:\n",
    "        no_header = response.text\n",
    "        # Process the FASTA text to remove the header line\n",
    "        lines = no_header.split('\\n')\n",
    "        # Skip the first line (header) and join the remaining lines to form the sequence\n",
    "        sequence_lines = lines[1:]  # This excludes the header\n",
    "        sequence = ''.join(sequence_lines)\n",
    "        return sequence\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "# Using the Pysam library\n",
    "# genome_dat = pysam.Fastafile('path/to/downloaded/reference_genome')\n",
    "\n",
    "\n",
    "\n",
    "#get consistent train, valid, test split\n",
    "random.seed(101)\n",
    "\n",
    "chrom_split,props = train_valid_test_split(filt_dna_bins, valid_frac=0.2, test_frac=0.2,tol=0.14)\n",
    "#add split column to dat\n",
    "for i in ['train','valid','test']:\n",
    "    filt_dna_bins.loc[filt_dna_bins['chr'].isin(chrom_split[i]),'split']=i\n",
    "\n",
    "# Now load and save the data as a np obj\n",
    "counters = {\"train\":0,\"valid\":0,\"test\":0}\n",
    "\n",
    "loop_once_for_example = False #\n",
    "\n",
    "for index, row in filt_dna_bins.iterrows():\n",
    "\n",
    "    if loop_once_for_example:\n",
    "        break\n",
    "\n",
    "    if(index%5_000==0 and index!=0):\n",
    "            print(row['chr'] +\": \"+str(row['pred_strt']))\n",
    "    split = row['split']\n",
    "    dna_strt = row['dna_strt']\n",
    "    dna_end = row['dna_end']\n",
    "    the_chr = row['chr']\n",
    "    all_y = load_y(data=bw_dat,\n",
    "                       labels=urls,\n",
    "                       selected_chromosome=the_chr,\n",
    "                       window_start=dna_strt,\n",
    "                       window_size = INPUT_WINDOW,\n",
    "                       target_bp = OUTPUT_WINDOW,\n",
    "                       pred_res = PRED_RES)\n",
    "    # Add the one-hot-encoded X values for the split region\n",
    "    X = one_hot_encode_sequence(genome_dat(the_chr, dna_strt, dna_end))\n",
    "    # X = one_hot_encode_sequence(genome_dat.fetch(the_chr, dna_strt, dna_end)) Using Pysam Library\n",
    "\n",
    "\n",
    "# Save each split as a .npz file\n",
    "\n",
    "    np.savez(f'{split}_{counters[split]}.npz',\n",
    "        X=X,\n",
    "        y=all_y)\n",
    "\n",
    "# Increment counter\n",
    "    counters[split]+=1\n",
    "\n",
    "    loop_once_for_example = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52d77b-ba82-4047-b2b4-3ba984c5b588",
   "metadata": {
    "id": "0c52d77b-ba82-4047-b2b4-3ba984c5b588"
   },
   "source": [
    "## Results\n",
    "\n",
    "Based on data from 3 chromosomes from 2 experiments, going through the process in full would result in 30446 training/validation/test positions available to load into a model. Each position has one-hot-encoded X values of shape (2048, 4) and coverage Y values of shape (1, 2) as seen below. Due to the large amount of splits, it is common to use a dataloader such as the PyTorch library DataLoader to handle batching, shuffling, and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eeb781-6b14-4f9e-8de7-d3ceffafb9f8",
   "metadata": {
    "id": "a1eeb781-6b14-4f9e-8de7-d3ceffafb9f8"
   },
   "outputs": [],
   "source": [
    "data = np.load(\"train_0.npz\")\n",
    "for key in data.files:\n",
    "    print(f\"Key: {key}\")\n",
    "    print(f\"Array shape: {data[key].shape}\")\n",
    "    print(data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf258dbe-9e51-4a07-9211-8ce32fa3c2a6",
   "metadata": {
    "id": "bf258dbe-9e51-4a07-9211-8ce32fa3c2a6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
