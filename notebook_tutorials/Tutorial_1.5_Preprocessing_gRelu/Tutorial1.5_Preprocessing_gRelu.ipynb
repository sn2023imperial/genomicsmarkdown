{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b96cad-3d9d-4006-9560-11037616f1cb",
   "metadata": {},
   "source": [
    "# **Tutorial 1.5. Preprocessing data for gRelu Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c6d99-1926-418a-9c7f-9f98d473ef8f",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/Genentech/gReLU/blob/main/README.md\" target=\"_blank\">gReLU</a> is a Python library to train, interpret, and apply deep learning models to DNA sequences\". As explained in the \"Software libraries for model building section\", the gRelu library contains a model zoo allowing for easy access to several models such as Borzoi, Enformer, or a dilated convolutional model based on the BPnet model architecture. Borzoi and Enformer can be imported pre-trained. Additionally, simpler base models and convolutional neural networks are also available. <br>\n",
    "\n",
    "This tutorial explains the pre-processing steps on data used to train gRelu models in Tutorial 2 (Training Models with gRelu and Examining Pitfalls). While the pre-processing steps in Tutorial 1 were done for a more general/imaginary model, processing data for gRelu models require slightly different naming conventions and dataset objects.\n",
    "\n",
    "To explain this process, this tutorial encorporates data pre-processing steps from Tutorial 1 (Loading and Pre-Processsing Data from bigWigs) with <a href=\"https://genentech.github.io/gReLU/tutorials/3_train.html#\" target=\"_blank\">gRelu's tutorial</a> on training their models. \n",
    "\n",
    "- gRelu's tutorial trains a single-task convolutional regression model to predict total coverage using ATAC-Seq data.\n",
    "- Models in Tutorial 2 also use a single-task convolutional regression model to predict total coverage.\n",
    "\n",
    "**Differences in approach:**\n",
    "\n",
    "| gRelu's Tutorial | Tutorial 2 |\n",
    "|----------|----------|\n",
    "| Uses ATAC-Seq Data as input   | Uses ChIP-Seq Data as input  |\n",
    "| Chromosomes 1-22    | Chromosomes 1-5**  |\n",
    "| Model trains on Centered Peaks    | Model trains on Thresholded Peaks (big difference in pre-processing)   |\n",
    "| Input Window: 2144bp    | Input Window: 2048bp  |\n",
    "| Prediction Resolution: 1000bp    | Prediction Resolution: 512bp  |\n",
    "| Aim: Understand how to train gRelu models    | Aim: Evaluating training decisions and examining pitfalls    |\n",
    "\n",
    "<br>\n",
    "** In discussing distributional differences in Tutorial 2, two models were trained on chromosomes 1-22. I repeated the pre-processing in this tutorial and Tutorial 1 using all chromosomes. The exact code and logic for every dataset is contained in several scripts named \"get_XYZ_dataset\". <br>\n",
    "<br>For continuation we will use data from the same ChIP-seq experiment from the Encode project used in the previous tutorial, <a href=\"https://www.encodeproject.org/experiments/ENCSR817LUF/\" target=\"_blank\">ENCSR817LUF</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602598c-d23b-4a05-9aab-50c595c62b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import grelu.data.preprocess\n",
    "import grelu.data.dataset\n",
    "import grelu.lightning\n",
    "import grelu.visualize\n",
    "import pickle\n",
    "import pyBigWig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ef20d-afed-4384-9890-26a71cbc1e10",
   "metadata": {},
   "source": [
    "<br>In order to reduce computational time in this tutorial, the first few steps from tutorial 1 are skipped over. These include creating a dataframe of 2048bp regions from chromosomes 1 through 5 and appending arcsinhed coverage values from the bigWig file, base pair averaged to 512bp. The resulting dataframe was saved into '512bpResolution_p_values.csv.gz'. The dataframe is available in the 'raw_data' folder (if downloaded locally) or available for download on Google Colab through the code below.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d4542-5553-4284-ad43-edc7f1c812f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!wget -O raw_data.zip 'https://www.dropbox.com/scl/fo/lcg4akvwi4ib8e9vey1re/ADgF_XwTy18Rend8ZB8YAbs?rlkey=0qgf8yt4exrgt4tu8qu16315d&st=p1dp3rx9&dl=0'\n",
    "\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('raw_data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('raw_data')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2bd16-3024-4d65-a9cd-8d3628a998fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.encodeproject.org/files/ENCFF601VTB/@@download/ENCFF601VTB.bigWig -O raw_data/ENCFF601VTB.bigWig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816ffd0-aa75-4e02-b7a5-e82de4af64a6",
   "metadata": {},
   "source": [
    "Model Constants\n",
    "-\n",
    "\n",
    "The models used in Tutorial 2 have an input window of 2048bp and a output window / prediction resolution of 512bp. This widens the resolution that the model trains on and predicts compared the hypothetical model in Tutorial 1 (32bp prediciton resolution). <br>\n",
    "<br>Note: gRelu's tutorial uses a similar input window but has a larger prediction resolution (1000bp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3718ac-8db8-480e-9188-35501b9fa504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants\n",
    "INPUT_WINDOW = 2048\n",
    "OUTPUT_WINDOW = 512\n",
    "PRED_RES = 512\n",
    "buffer_bp = (INPUT_WINDOW-OUTPUT_WINDOW)//2\n",
    "val_chroms = \"chr3\"\n",
    "test_chroms = \"chr2\"\n",
    "genome = \"hg38\"\n",
    "bw_file =  'raw_data/ENCFF601VTB.bigWig' #p-value \n",
    "\n",
    "\n",
    "# Model will predict on chromsomes 1\n",
    "CHROMOSOMES =np.array(['chr1', 'chr2', 'chr3', 'chr4', 'chr5'])\n",
    "\n",
    "\n",
    "# hg38 chrom lengths\n",
    "# Human Genome Assembly GRCh38.p14\n",
    "CHROM_LEN =np.array([248_956_422, 242_193_529, 198_295_559, 190_214_555, 181_538_259, 170_805_979, \n",
    "                     159_345_973, 145_138_636, 138_394_717, 133_797_422, 135_086_622, 133_275_309,\n",
    "                     114_364_328, 107_043_718, 101_991_189, 90_338_345, 83_257_441, 80_373_285,\n",
    "                     58_617_616, 64_444_167, 46_709_983, 50_818_468])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0d189-2ffd-4c9f-8a42-c024d0c434a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our DNA bins filled with our target values\n",
    "dna_bins = pd.read_csv('raw_data/512bpResolution_p_values.csv.gz')  # Intervals and coverage from chromosome's 1-5 with a 512bp resolution (summed)\n",
    "\n",
    "# Set a threshold to filter our data\n",
    "threshold = 2 # A threshold of 2 coming from research explained in Tutorial 1\n",
    "thresholdarc = np.arcsinh(threshold) # The threshold has to undergo an arcsinh transformation as well\n",
    "\n",
    "filt_dna_bins = dna_bins.loc[dna_bins['cov']>thresholdarc].reset_index(drop=True) # Apply the threshold to all chromosomes\n",
    "print(f\"{filt_dna_bins.shape[0]} training/validation/test positions.\\n\")\n",
    "print(filt_dna_bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f3515-deaf-479c-8ec3-b823e5540eea",
   "metadata": {},
   "source": [
    "Recap\n",
    "-\n",
    "\n",
    "'filt_dna_bins' now contains 2048bp regions from chromosome 1 through 5 where the coverage values are above our threshold. While the dataframe contains the actual coverage values, gRelu's pre-processing functions retrieve them from the bigWig when creating final training/validation/test sets. What matters here are the **regions** that we have thresholded as peaks, e.g. thresholded peaks. We can simply let gRelu's function retrieve the coverage values later on in the process. This is why the next step is to keep only the **thresholded peak regions**. <br>\n",
    "\n",
    "In gRelu's tutorial, peak regions are retrieved from a narrowpeak file following peak calling with MACS2.<br>\n",
    "<br><img src=\"narrowpeak.png\" alt=\"Alt Text\" width=\"600\"/>\n",
    "<br><br>\n",
    "As you can see these peak regions vary in size, [206bps, 182bps, 150bps]. The next step in the gRelu tutorial is to create **centered peaks regions**. gRelu has a 'grelu.data.preprocess.extend_from_coord' function which returns a dataframe of regions surrounding the summit of each peak spanning X base pairs long (in this case their input window 2114bps). <br>\n",
    "\n",
    "These are the two main methods of determining which regions to use as peak regions. While either approach is valid, as explained in the 'Training Tricks\" section of the markdown book, <a href=\"https://www.nature.com/articles/s42256-022-00570-9\" target=\"_blank\">research</a> has found that thresholded peaks benefit from randomly-centered profiles making convolutional models trained on them more robust without the need for sequence shift augmentations. (Tutorial 2 will explore reverse complement augmentation on the thresholded peak models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6d4e7-ffe1-4687-a629-1da22cf48f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the 'chrom', 'start', and 'end' columns e.g. our thresholded peak regions\n",
    "peaks = filt_dna_bins[['chrom', 'start', 'end']]\n",
    "print(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221f616-75d9-4e17-a8f0-ac7e890cce67",
   "metadata": {},
   "source": [
    "<br>\n",
    "Here we utilise gRelu's 'filter_blacklist function' which filters out regions if they are within 50bp of a blacklist region. Blacklist regions are regions across the genome which consistenly show high signals irrespective of the experiment conducted, leading to increases in false-positive peaks.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39deae1b-c175-4815-97dd-1be746124a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = grelu.data.preprocess.filter_blacklist(\n",
    "        peaks,\n",
    "        genome=genome, #hg38\n",
    "        window=50 # Remove peaks if they are within 50 bp of a blacklist region\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b5321-6a00-46dc-b6ed-02dca7f6b20c",
   "metadata": {},
   "source": [
    "\n",
    "Appending Non-Peak Regions to our Data\n",
    "-\n",
    "\n",
    "In Tutorial 1, after pre-processing our bigWig data, we were left with thresholded peaks (both regions and coverage). If we trained our model solely on these datapoints, we would be falling into one of the pitfalls, unbalanced classes. We need to provide our model with example datapoints of non-peak regions. gRelu's 'grelu.data.preprocess.get_gc_matched_intervals' not only provides non-peak regions but it ensures the non-peak regions have a similar GC (the proportion of guanine (G) and cytosine (C) nucleotides) content to our peaks. GC content bias can arise from high thoroughput sequencing experiments such as ATAC-Seq and ChIP-Seq. A <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3639258/\" target=\"_blank\">research paper</a> explains that \"sequencing data is considered as GC biased if more (or less) reads tend to come from a region with a higher GC content.\" Maintaining a similar GC content in our non-peak regions prevents this bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8fc05c-b650-4e73-99fd-fb0dad8d919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = grelu.data.preprocess.get_gc_matched_intervals(\n",
    "    peaks,\n",
    "    binwidth=0.02, # resolution of measuring GC content\n",
    "    genome=genome,\n",
    "    chroms=['chr1', 'chr2', 'chr3', 'chr4', 'chr5'], \n",
    "    blacklist=genome, # negative regions overlapping the blacklist will be dropped\n",
    "    seed=0,\n",
    "    )\n",
    "negatives.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004efa5-0291-4784-88eb-9e1b781ee49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grelu.visualize\n",
    "grelu.visualize.plot_gc_match(\n",
    "        positives=peaks, negatives=negatives, binwidth=0.02, genome=\"hg38\", figsize=(4, 3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f87686-87bf-4c38-9e2c-be2be1659808",
   "metadata": {},
   "source": [
    "<br>\n",
    "This visualisation shows the GC content in both the peaks and non-peak regions so we can ensure they are similar.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172dbf5-fbc0-4d30-87dd-eaef5ddc09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining our peak and non-peak data\n",
    "regions = pd.concat([peaks, negatives]).reset_index(drop=True)\n",
    "len(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9841b-bfdb-4ad2-8deb-cd4ed647a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa7d54-202c-49c5-95bd-a797781a6658",
   "metadata": {},
   "source": [
    "Resampling\n",
    "-\n",
    "\n",
    "The models trained on chromosomes 1-5 in Tutorial 2 to demonstrate concepts have been trained with minimal data (15,000) for computational and time constraints. Sklearn's resample has been used to resample and split datasets into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9e052-e0a1-4832-a4c2-ea9a69554d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "train_size = 15000\n",
    "valid_size = 1500\n",
    "test_size = 1500\n",
    "\n",
    "# 1. Filter the data by chromosome\n",
    "chr2_data = regions[regions['chrom'] == test_chroms]\n",
    "chr3_data = regions[regions['chrom'] == val_chroms]\n",
    "train_data = regions[~regions['chrom'].isin([test_chroms, val_chroms])]\n",
    "\n",
    "# 2. Downsample chr2 and chr3 data if necessary\n",
    "if len(chr2_data) > test_size:\n",
    "    chr2_data = resample(chr2_data, n_samples=test_size, random_state=1)\n",
    "\n",
    "if len(chr3_data) > valid_size:\n",
    "    chr3_data = resample(chr3_data, n_samples=valid_size, random_state=1)\n",
    "\n",
    "# 3. Downsample the training data to 12,000 if necessary\n",
    "if len(train_data) > train_size:\n",
    "    train_data = resample(train_data, n_samples=train_size, random_state=1)\n",
    "\n",
    "# 4. Combine the final training, validation, and test sets\n",
    "train = train_data\n",
    "val = chr3_data\n",
    "test = chr2_data\n",
    "\n",
    "# 5. Print the sizes of each split to verify\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(val))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426eb94-c843-46c0-80b0-316b59cef214",
   "metadata": {},
   "source": [
    "Creating our gRelu datasets\n",
    "-\n",
    "Here we use gRelu's \"grelu.data.dataset.BigWigSeqDataset\" function to create our final training, validation and test sets. These datasets are of a custom object type, made for gRelu models. Here, we input our thresholded peak regions (train) as the intervals. gRelu's function retrieves the labels from the bw_file ('ENCFF601VTB.bigWig'), aggregating the central 512bps (PRED_RES). Instead of averaging the central 512bp coverage values, models in Tutorial 2 are trained on the summed coverage values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56352a-00c5-4ddf-a879-0eb6b54fb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grelu.data.dataset\n",
    "    \n",
    "train_ds = grelu.data.dataset.BigWigSeqDataset(\n",
    "    intervals = train,\n",
    "    bw_files=[bw_file],\n",
    "    label_len=PRED_RES,\n",
    "    label_aggfunc=\"sum\",\n",
    "    #rc=True, # reverse complement\n",
    "    #max_seq_shift=2, # Shift the sequence\n",
    "    #augment_mode=\"random\",\n",
    "    seed=0,\n",
    "    genome=genome,\n",
    "    label_transform_func=np.arcsinh\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade404e4-0350-4be9-9bc3-3e3fe538bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = grelu.data.dataset.BigWigSeqDataset(\n",
    "    intervals = val,\n",
    "    bw_files=[bw_file],\n",
    "    label_len=PRED_RES,\n",
    "    label_aggfunc=\"sum\", \n",
    "    genome=genome,\n",
    "    label_transform_func=np.arcsinh\n",
    ")\n",
    "\n",
    "test_ds = grelu.data.dataset.BigWigSeqDataset(\n",
    "    intervals = test,\n",
    "    bw_files=[bw_file],\n",
    "    label_len=PRED_RES,\n",
    "    label_aggfunc=\"sum\",\n",
    "    genome=genome,\n",
    "    label_transform_func=np.arcsinh\n",
    ")\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0d2a2-b08a-4bca-be0b-085156e7c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a184fc-eb07-4605-b85c-40238550ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ddd15-f3ea-48c1-af1a-e77826a3b722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754287d-c918-4639-8542-9682c4440476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77d7704c-5e40-40f8-9398-8f176de9df88",
   "metadata": {},
   "source": [
    "Distribution of peaks in our datasets vs chromosomes\n",
    "-\n",
    "\n",
    "As a result of thresholded peaks and the addition of non-peak regions, we have effectively downsampled the majority class (non-peaks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171292e-7187-40bd-8d9c-f1b3c929e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of train_ds vs chromosomes 1, 4, 5 distribution.\n",
    "labels = np.array(train_ds.get_labels()).reshape(-1)\n",
    "# Reversing transformations of train_ds labels + thresholding\n",
    "# e.g. reverse arcsinh, divide by PRED_RES (we thresholded peaks by averaging, not summing)\n",
    "thresholded_labels = np.where(np.arcsinh(np.sinh(labels) //PRED_RES)>= thresholdarc, 1, 0)\n",
    "num_ones = np.sum(thresholded_labels)\n",
    "\n",
    "print(f\"% of peaks in train_ds : {num_ones*100/(len(train_ds)):.1f}%\")\n",
    "\n",
    "\n",
    "# Apply the threshold to unfiltered training data\n",
    "training_chroms = dna_bins[(dna_bins['chrom'] != test_chroms) & (dna_bins['chrom'] != val_chroms)]\n",
    "thresh_label = np.where(training_chroms['cov'] >= threshold, 1, 0)\n",
    "# Calculate the number of peaks in training chromosomes\n",
    "num_peaks_train_chroms = np.sum(thresh_label)\n",
    "percentage_peaks_chroms = num_peaks_train_chroms * 100 / len(training_chroms)\n",
    "\n",
    "print(f\"% of peaks in real distribution training chroms (1, 4, 5): {percentage_peaks_chroms:.1f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b438d-19da-4cf2-bf56-483ea3f94d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of val_ds vs chromosome 3s distribution\n",
    "labels = np.array(val_ds.get_labels()).reshape(-1)\n",
    "thresholded_labels = np.where(np.arcsinh(np.sinh(labels) //PRED_RES)>= thresholdarc, 1, 0)\n",
    "num_ones = np.sum(thresholded_labels)\n",
    "\n",
    "print(f\"% of peaks in val_ds : {num_ones*100/(len(val_ds)):.1f}%\")\n",
    "\n",
    "\n",
    "# Apply the threshold to unfiltered chrom3 data\n",
    "chrom3_data = dna_bins[dna_bins['chrom'] == val_chroms]\n",
    "thresh_label = np.where(chrom3_data['cov'] >= threshold, 1, 0)\n",
    "# Calculate the number of peaks in chrom3 data\n",
    "num_peaks_chrom3 = np.sum(thresh_label)\n",
    "percentage_peaks_chrom3 = num_peaks_chrom3 * 100 / len(chrom3_data)\n",
    "\n",
    "print(f\"% of actual peaks in val chrom (3): {percentage_peaks_chrom3:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d53c12-bf80-44e8-a875-ccd8cd58b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of test_ds vs chromosome 2s distribution\n",
    "labels = np.array(test_ds.get_labels()).reshape(-1)\n",
    "thresholded_labels = np.where(np.sinh(labels) //PRED_RES>= threshold, 1, 0)\n",
    "num_ones = np.sum(thresholded_labels)\n",
    "\n",
    "print(f\"% of peaks in test_ds : {num_ones*100/(len(test_ds)):.1f}%\")\n",
    "\n",
    "\n",
    "# Apply the threshold to chrom2_data\n",
    "chrom2_data = dna_bins[dna_bins['chrom'] == test_chroms]\n",
    "thresh_label = np.where(chrom2_data['cov'] >= threshold, 1, 0)\n",
    "# Calculate the number of peaks in chrom2_data\n",
    "num_peaks_chrom2 = np.sum(thresh_label)\n",
    "percentage_peaks_chrom2 = num_peaks_chrom2 * 100 / len(chrom2_data)\n",
    "\n",
    "print(f\"% of actual peaks in test chrom (2): {percentage_peaks_chrom2:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0b1c2-fb48-4362-84e9-0fa0b6ed4119",
   "metadata": {},
   "source": [
    "Visualising our datasets\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7c0d5-23fd-43d6-9d24-eaf9edcd0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_distribution(labels, title):\n",
    "    labels_flat = labels.flatten()  # Flatten the labels to 1D\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(labels_flat, bins=50, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribution of {title}')\n",
    "    plt.xlabel('Target Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Print basic statistics\n",
    "def print_statistics(labels, title):\n",
    "    labels_flat = labels.flatten()  # Flatten the labels to 1D\n",
    "    print(f\"Statistics for {title}:\")\n",
    "    print(f\"Mean: {np.mean(labels_flat):.2f}\")\n",
    "    print(f\"Median: {np.median(labels_flat):.2f}\")\n",
    "    print(f\"Standard Deviation: {np.std(labels_flat):.2f}\")\n",
    "    print(f\"Min: {np.min(labels_flat):.2f}\")\n",
    "    print(f\"Max: {np.max(labels_flat):.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e564cf-bcf9-4bca-abf8-1d7b69740da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_ds.get_labels())\n",
    "plot_distribution(train_labels, 'Training Dataset')\n",
    "print_statistics(train_labels, 'Training Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754df31-2b5c-4a1f-bbef-8b90fb941fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = np.array(val_ds.get_labels())\n",
    "plot_distribution(val_labels, 'Validation Dataset')\n",
    "print_statistics(val_labels, 'Validation Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85430b-df8c-4838-aaef-88f0662c4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.array(test_ds.get_labels())\n",
    "plot_distribution(test_labels, 'Test Dataset')\n",
    "print_statistics(test_labels, 'Test Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec69489-ec9f-4a87-9db9-c8e31eb491fe",
   "metadata": {},
   "source": [
    "<br>\n",
    "The distributions are skewed towards peaks as we have effective downsampled the majority class (non-peaks), so that our model has more examples of peaks to train on. gRelu's tutorial ends with these 50-50 peak/non-peak datasets being used to train a convolutional regression model. Tutorial 2 will explore the effects of training a model in this way.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52866f-2a50-410c-9f9a-00130563a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our datasets as pickle files\n",
    "if not os.path.exists('datasets'):\n",
    "    os.makedirs('datasets')\n",
    "\n",
    "with open('datasets/train_ds_balanced.pkl', 'wb') as f:\n",
    "    pickle.dump(train_ds, f)\n",
    "\n",
    "\n",
    "with open('datasets/val_ds_balanced.pkl', 'wb') as f:\n",
    "    pickle.dump(val_ds, f)\n",
    "\n",
    "\n",
    "with open('datasets/test_ds_balanced.pkl', 'wb') as f:\n",
    "    pickle.dump(test_ds, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe7231-2c40-4b59-96ca-bf2ed11a1b3d",
   "metadata": {},
   "source": [
    "<br>\n",
    "To explain several pitfalls in Tutorial 2, datasets were made using these functions and logic using the following scripts:\n",
    "<br><br>\n",
    "\"get_allpeaks_datasets.py\" <br>\n",
    "\"get_real_distribution_datasets.py\" <br>\n",
    "\"get_balanced_datasets.py\" (this Tutorial) <br>\n",
    "\"get_unsplit_datasets.py\" <br>\n",
    "\"get_compare_datasets.py\" <br>\n",
    "\"get_unsplit_all_chroms_datasets.py\" <br>\n",
    "\"get_allchroms_compare_dataset.py\" <br>\n",
    "<br>\n",
    "For reproducibility purposes, these scripts are included in this tutorial's repository (if downloaded locally) or available to be downloaded through the code below\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08a077-e40d-432b-9f82-a60420107cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!wget -O dataset_scripts.zip 'https://www.dropbox.com/scl/fo/hii7mrxhopz27f9c4z4oh/AFp_kbskmR7uiGzNMPRdkqc?rlkey=78s482a0o62x42uv1kmj6i3v9&st=8eua7xwc&dl=0'\n",
    "\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('dataset_scripts.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('dataset_scripts')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba0d8f-15c5-4e11-8b98-744f25f17410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 dataset_scripts/get_allpeaks_datasets.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
