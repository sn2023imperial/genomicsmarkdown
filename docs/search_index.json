[["index.html", "Machine Learning in Genomics: Containerised tutorials demonstrating best practises, pitfalls, and reproducibility About", " Machine Learning in Genomics: Containerised tutorials demonstrating best practises, pitfalls, and reproducibility Sach Nehal 2024-09-03 About Applied machine learning utilising vast amounts of data has aided in pattern identification, predictive analytics, and solving complex problems across a multitude of fields. Solving these complex problems within these fields, researchers would find differing answers to the following questions; what machine learning techniques can we apply to the problem, how do we apply the techniques in the context of this field, and why do we need to apply them in this way? In any case, applied machine learning requires an interdisciplinary understanding of computing techniques and the field in question. The aim of this project is to provide you with a set of reproducible tutorials that include all necessary data, code, and descriptions to replicate key results, along with demonstrations of common pitfalls, in the field of genomics. It is designed for users with knowledge of machine learning but little or no background in biology as a process to learn about applying machine learning techniques in genomics. "],["epigenetic-data.html", "1 Epigenetic Data 1.1 What is epigenetic data? 1.2 What does epigenetic data look like? 1.3 Sources of epigenetic data 1.4 UCSC’S Genome Browser", " 1 Epigenetic Data 1.1 What is epigenetic data? As you may already know, typically all of the cells in your body contain the same DNA. How, then, do we have different cell types in our body? Your DNA contains a script that is able to produce the proteins required for each specific cell in your body. Which proteins, and subsequently which cells are made, depends on gene expression and regulation, i.e. “the way each cell deploys its genome.”1 Epigenetic data arises from “the study of heritable and stable changes in gene expression that occur through alterations in the chromosome rather than in the DNA sequence.”2 commonfund.nih.gov The image above shows quite simply the basics of genetic structures. Several more complex processes are involved during cell replication such as DNA transcription and translation in order to make proteins. A key takeaway in coming closer to understanding gene expression is that Chromatin is a complex structure made up of DNA wound around histone proteins, with some segments of DNA being accessible/inaccessible to further processes. Euchromatin refers to the accessible state, while Heterochromatin refers to a chromatin state in which DNA cannot be transcribed (inaccessible).3 There are many different epigenetic modifications that affect chromatin accessibility. Some common epigenetic modifications include: DNA Methylation: Addition of methyl groups to DNA, affecting gene expression regulation4. Histone Modifications: Chemical changes to histone proteins that DNA wraps around, including acetylation, methylation, or phosphorylation. These changes influence chromatin structure and gene accessibility.5 Chromatin Accessibility: Regions of open chromatin that are accessible to transcription factors (special types of proteins that bind to DNA sequences and regulate gene expression) further dictate which regions of DNA can be expressed6. In studying gene expression and epigenetic modifications, we aim to more closely understand biological mechanisms that regulate development, disease, and how cells respond to epigenetic factors. 1.1.1 What Does DNA Look Like? As illustrated in the image above, DNA is structured as a double helix, with two complementary strands intertwined to form the characteristic helical shape. DNA consists of an extremely long sequence composed of four types of nucleotides: Adenine (A), Cytosine (C), Thymine (T), and Guanine (G). According to the National Cancer Institute (USA), nucleotides within the DNA double helix form complementary pairs—Adenine pairs with Thymine, and Guanine pairs with Cytosine7. These pairs are commonly referred to as base pairs (bps). For example, if one strand of the double helix has the sequence “ATCGG”, the complementary strand will have the sequence “TAGCC”. Genes are sequences of DNA located at specific positions on chromosomes and can vary in length. Each gene encodes information necessary for producing proteins or RNA molecules, which are essential for the structure, function, and regulation of an organism8. The complete set of genetic material in an organism is known as its genome. Image highlighting part of a dna sequence and base pairs.9 1.1.2 Common Epigenetic Sequencing Techniques: ATAC-Seq (Assay for Transposase-Accessible Chromatin with Sequencing): oMeasures chromatin accessibility to identify open regions of the genome where transcription factors can bind. oOutput: Peaks indicating accessible chromatin regions. ChIP-Seq (Chromatin Immunoprecipitation Sequencing): oUsed to identify DNA regions bound by specific proteins (e.g., transcription factors, histones with specific modifications). oOutput: Peaks indicating binding sites or modification locations. 1.2 What does epigenetic data look like? Epigenetic data can be represented in various forms, depending on the type of modification being studied and the methods used to gather the data. ATAC-Seq and ChIP-Seq are the common methods I will focus on, but there are others that may produce different forms of data, such as WGS (whole-genome sequencing) which produces nucleotide sequencing data, or Bisulfite conversion of DNA producing data on methylation levels across the genome. 1.2.1 Representing epigenetic data Epigenetic data originates from sequencing methods such as ATAC-Seq or ChIP-Seq experiments. The initial experiments produce raw sequencing reads (fragments), which are then aligned to a reference genome. By aligning these sequences, we can aggregate the reads into regions where they ‘pile up’ to form peaks, indicating areas of significant biological activity or modification. This can be done per base across the genome, or per gene. Additionally, we could also examine mismatches where a read’s base differs from the genome’s base, and use them to identify SNPs (single nucleotide polymorphisms)10. This mismatch information can be recorded in a table showing the position, type of mismatch, and the number of reads supporting each mismatch.11 Image showing the sequencing pipeline from high-throughput sequencing methods12. 1. Raw Sequence Reads: oThese are the basic output of sequencing experiments, such as those from ChIP-Seq or ATAC-Seq. oReads are processed and aligned to a reference genome before undergoing peak calling. Lets look at what a few lines of raw sequence read data consists of: The data is taken from Encode Experiment ENCSR817LUF (chIP-Seq). The accession ID of the raw sequence read data is ENCFF397NRK. Genomic data comes in many file formats. This specific raw sequence read data is a compressed FASTQ file. Note: The script I used involved streaming the data directly from a URL using the requests library. While files containing genomic data are generally quite large, for computational efficiency it is recommended that data be downloaded locally. ## ID: B091JABXX110402:1:2204:12975:184709 ## Sequence: GTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGG ## Quality Scores: [31, 30, 30, 32, 36, 37, 36, 32, 33, 32, 35, 36, 37, 33, 33, 34, 37, 37, 36, 33, 34, 30, 37, 37, 37, 33, 34, 33, 38, 33, 33, 32, 33, 35, 37, 36] ## ## ID: B091JABXX110402:1:2205:18641:8399 ## Sequence: GGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAG ## Quality Scores: [22, 27, 31, 30, 30, 33, 31, 32, 31, 31, 24, 36, 37, 36, 33, 34, 33, 37, 37, 32, 32, 34, 22, 37, 36, 30, 23, 35, 27, 29, 28, 28, 23, 31, 26, 35] ## ## ID: B091JABXX110402:1:1207:12202:100922 ## Sequence: AGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTT ## Quality Scores: [33, 33, 36, 37, 31, 34, 34, 36, 37, 36, 29, 33, 35, 36, 39, 37, 32, 33, 35, 37, 34, 37, 33, 34, 36, 38, 39, 25, 32, 35, 36, 38, 37, 38, 32, 32] As you can see, each data entry is a DNA sequence (read). While I’m only showing the first three entries, for each experiment there are millions of sequencing reads. The quality scores indicate the confidence of each base call in the sequence. Higher scores suggest higher confidence. The scores are in Phred format, where a score of 20 corresponds to a 99% base call accuracy, 30 corresponds to 99.9%, 40 corresponds to 99.99%, and so on.13 2. Peak Calling: oA method used to identify regions in the genome where there is significant enrichment of sequencing reads. This indicates the presence of DNA-protein interactions (e.g., transcription factor binding sites or accessible chromatin regions). oPeaks represent areas where epigenetic marks or chromatin accessibility are concentrated. A common peak calling algorithm is MACS2. Essentially, aligned sequencing reads are aggregated into regions where they ‘pile up’ to form peaks as a read count per base. The outputs typically include signal p-values or fold change over control values, representing the expectation of a peak. Signal p-values are negative log transformed resulting in -log10 signal p-values. While MACS2 is traditionally used in ChIP-seq experiments, it is also applied to ATAC-seq to identify significant peaks and assess their enrichment.14 In ChIP-seq, broad peaks often represent histone modifications covering entire gene bodies, while narrow peaks are indicative of transcription factor binding sites. In ATAC-seq, the peaks primarily reflect regions of open chromatin.15 Peak calling reduces background noise and utilises signal smoothing techniques to more accurately detect peaks. When using -log10 p-value and fold change data, base pair averaging is commonly used. Imagine you have a set of read coverage data from a sequencing experiment. At each base pair position along a chromosome, you have a read count representing how many times that base pair was sequenced. However, due to technical noise, these counts might fluctuate wildly from one base to the next. By applying base pair averaging over a window (e.g. 32bp), you might see that while individual counts vary, there is a broader region where the average read coverage is consistently high, indicating a potential region of interest. The intended effect of base pair averaging is further reducing noise, and signal smoothing. Figure showing a 32 base pair resolution following base pair averaging of CHip-seq data16 As you will see in the tutorials, you can re-average your data to a higher base pair average before using it in machine learning models. This can be implemented to decrease dimensionality, reduce computational intensity and tailor the model to understanding regions of a certain scale. Representing Peaks: o P-value or Fold-change: P-value: Indicates the statistical significance of the peak, helping to distinguish true peaks from background noise. Fold-change: Represents the difference in read density between treated and control samples, indicating the strength of the signal. This image shows the signal p-value coverage over a small region (11,176bps) in chromosome one from Encode Experiment ENCSR817LUF (The same chIP-Seq experiment we saw the raw sequence reads from). For further context, experiment ENCSR817LUF targets the H3K36me3 histone modification in brain tissue. The experiment aims to map the locations where the H3K36me3 histone modification is present along the genome. Therefore the peaks represent regions of the genome where the the H3K36me3 histone modification is enriched compared to the background or control. The accession ID of the signal p-value data is ENCFF601VTB. Genomic data comes in many file formats. This specific signal p-value data is a bigWig file. o Types of Peaks: Categorical Peaks: Simple yes/no indication of a peak’s presence. Continuous Peaks: More nuanced representation that includes the intensity or enrichment level of the peak, often visualized as a signal track. Thresholded/Pseudoreplicated Peaks: Usually categorical, these peaks are of high confidence regions from multiple replicates (experiments) or pseudoreplicates (artificial data splits), to ensure reliability and reproducibility. Example Data Pipeline encodeproject.org This is an example data pipeline from Encode Experiment ENCSR817LUF, the same chIP-Seq experiment we saw the raw sequence reads, and signal p-value coverage from. The yellow bubbles represent downloadable data sets of different types, while the blue boxes represent step types (e.g. peak calling). In the left column are multiple data sets of raw sequence reads, which then undergo data quality steps before being aligned (first blue box) to the reference human genome GRCh38 (denoted by ENCFF110MCL below the reads). The next steps include Peak calling (categorical peaks) and signal generation (continuous peaks) to produce the data we normally use in our machine learning models. This data pipeline process aids in normalisation, noise reduction, and dimensionality reduction of the data. 1.2.2 Transformations to stop extreme p-values When utlising genomic data which incorporates p-values, it is important to consider and deal with extreme p-values. One way this is done is through using an Arcsinh-transformation (inverse hyperbolic sine). \\(\\text{arsinh}(x) = \\ln \\left( x + \\sqrt{x^2 + 1} \\right)\\) The arcsinh-transformation as a logarithmic function helps in reducing the significance of outliers and sequencing depth while maintaining variance by compressing the range of the data. This transformation can be used in the data preprocessing stage. The graph below visualises how the transformation works. While extreme values are transformed logarithmically, the smaller values are barely transformed as the function for smaller values is more linear in nature. Plot of Arcsinh Transformation compared to a log function, made with Desmos available under CC BY-SA 4.0. Text, arrows, and box shape added to image. 1.3 Sources of epigenetic data There are numerous public data banks which contain genomic datasets ready to be downloaded. Blueprint Blueprint’s genomic datasets are focused on gene expression in healthy and diseased cells mostly relating to haematopoietic cells (cells which develop into different types of blood cells). Roadmap The National Institute of Health’s Roadmap Epigenomics Project contains sample datasets from multiple experiements as well as reference and mapping datasets. Encode The Encode Project contains a large amount of publicly available genomic data easily filtered and downloaded. The genomic data used in this markdown book is sourced from Encode. The largest genomic data bank is the UK Biobank, however they require that you apply for access to their datasets. 1.4 UCSC’S Genome Browser The UCSC Genome Browser is a powerful and versatile tool that allows the visualisation and exploration of many sets of genomic data, especially bigWig files. It offers an extensive collection of genome assemblies, annotation tracks, and functional data, enabling users to examine gene structures, regulatory elements, and genetic variations. With its user-friendly interface and customisable display options, the UCSC Genome Browser facilitates detailed genomic analyses and supports a wide range of applications in genomics and bioinformatics. Whether you’re investigating gene functions, exploring genetic variants, or studying comparative genomics, the UCSC Genome Browser serves as an essential resource for understanding complex genomic information. It is also possible to load and visualise genomic data from other sources such as Encode. While the visualisations are extensive, as you can explore below, the browser can be quite overwhelming for first time users. The following is an example of what the same chIP-Seq data targeting the H3K36me3 histone modification in brain tissue looks like using UCSC’s Genome Browser. The pseudoreplicated peaks represent categorically, the significant locations along the genome where the H3K36me3 histone modification is present. UCSC Genome Browser The following is an example of ATAC-Seq data from an experiment on T-helper 17 cells (a type of immune system cell). Recall that the ATAC-Seq method aims to find chromatin regions that are accessible for transcription factor binding. The p-value and fold change graphs show continuous peaks, while the IDR thresholded peaks and pseudoreplicated peaks represent the significant locations of accessible chromatin along the genome. UCSC Genome Browser References Akalin, Altuna. 2020. Computational Genomics with r. Github Pages. https://compgenomr.github.io/book/. Al-Aboud, Nora M., Connor Tupper, and Ishwarlal Jialal. 2023. Genetics, Epigenetic Mechanism. National Library of Medicine. https://www.ncbi.nlm.nih.gov/books/NBK532999/#article-22137.r1. Board, PDQ® Cancer Genetics Editorial. n.d. Genetics-Dictionary. National Cancer Institute USA. https://www.cancer.gov/publications/dictionaries/genetics-dictionary. Green, Phil, and Brent Ewing. n.d. Phred - Quality Base Calling. CodonCode Corporation. https://www.phrap.com/phred/#:~:text=Phred%20is%20a%20base%2Dcalling,%22)%20to%20each%20base%20call. Kanani, Pratik, and Mamta Padole. 2020. Improving Pattern Matching Performance in Genome Sequences Using Run Length Encoding in Distributed Raspberry Pi Clustering Environment. Procedia Computer Science. https://www.sciencedirect.com/science/article/pii/S1877050920311601?via%3Dihub. Kappelmann-Fenzl, Melanie. 2021. Design and Analysis of Epigenetics and ChIP-Sequencing Data. Springer. https://doi.org/10.1007/978-3-030-62490-3_12. Mistry, Meeta, Jihe Liu, Radhika Khetani, and et al. 2022. Peak Calling with MACS2. Github Pages. https://hbctraining.github.io/Intro-to-ChIPseq-flipped/lessons/06_peak_calling_macs.html. Patel, Jai. 2024. Leveraging Genetic Diversity with Machine Learning. Imperial College London. Ralston, Amy, and Kenna Shaw. 2008. Gene Expression Regulates Cell Differentiation. Nature Education. https://www.nature.com/scitable/topicpage/gene-expression-regulates-cell-differentiation-931/#:~:text=All%20of%20the%20cells%20within,each%20cell%20deploys%20its%20genome. Shahid, Zainab, Brittany Simpson, Kathleen H. Miao, and Gurdeep Singh. 2023. Genetics, Histone Code. StatPearls Publishing LLC. https://www.ncbi.nlm.nih.gov/books/NBK538477/. T., Kouzarides. 2007. Chromatin Modifications and Their Function. National Library of Medicine. https://doi.org/10.1016/j.cell.2007.02.005. Wilbanks, Elizabeth, and Marc Facciotti. 2010. Evaluation of Algorithm Performance in ChIP-Seq Peak Detection. Plos One Journal. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0011471. Ralston and Shaw (2008)↩︎ Al-Aboud, Tupper, and Jialal (2023)↩︎ Shahid et al. (2023)↩︎ Al-Aboud, Tupper, and Jialal (2023)↩︎ T. (2007)↩︎ Kappelmann-Fenzl (2021)↩︎ Board (n.d.)↩︎ Board (n.d.)↩︎ Kanani and Padole (2020)↩︎ Board (n.d.)↩︎ Akalin (2020)↩︎ Akalin (2020)↩︎ Green and Ewing (n.d.)↩︎ Mistry et al. (2022)↩︎ Wilbanks and Facciotti (2010)↩︎ Patel (2024)↩︎ "],["pre-processing-of-bigwig-files.html", "2 Pre-processing of bigWig files 2.1 Data loaders and simplifying pre-processing 2.2 Dealing with missing data (oversampling, undersampling, weighting)", " 2 Pre-processing of bigWig files BigWig files containing signal p-value or fold change data can be quite tricky to deal with. However, libraries such as pyBigWig enable easier access of data. In order to understand how to handle the data pre-processing stage, I have created a jupyter notebook tutorial on Google Colab. The tutorial begins using UCSC’s programs to quickly understand the genomic data within BigWigs, before using the pyBigWig library to simply extract BigWig data. The final part of the tutorial uses the pyBigWig library to load, filter, and split BigWig data into training, validation, and test sets. The data consists of signal p-values from ChIP-seq experiments, processed using the MACS2 tool. We will re-average these signals to a resolution of 32 base pairs. Additionally, we will implement threshold-based filtering and consistent data splits to understand how to ready data for a model. Tutorial 1: Loading and Pre-Processsing Data from bigWigs (interactive) Tutorial 1: Loading and Pre-Processsing Data from bigWigs (nbviewer) 2.1 Data loaders and simplifying pre-processing Data loaders are scripts/functions to load batches of data into your model. They are crucial in machine learning because they simplify how data is fed into models, making the whole process smoother and more efficient. This becomes especially important with the large datasets used in genomic studies, where managing and processing data manually would be cumbersome. By automating these tasks, data loaders help ensure that data is processed efficiently, allowing for faster and more effective model training. While there are existing github repositories with data loaders, such as “Kipoi Dataloader”, and “Dataloader for BigWig files”.17, depending on the data used and model you build, they won’t cover all of the use cases. When building one yourself, the PyTorch library has its own dataset and dataloader modules, which include creating custom datasets. 2.2 Dealing with missing data (oversampling, undersampling, weighting) In genomics, class imbalance is a frequent challenge, often necessitating the use of statistical methods to validate the few positive instances amid vast amounts of data. This is particularly evident in tasks such as alignment queries, GWAS projects, and motif scanning, where conservative significance thresholds are essential to control false positives due to the low frequency of true positives across the genome. Researchers tend to address these imbalances by either oversampling the minority class, undersampling the majority class or by employing weighting.18 In Tutorial 1, we utilised thresholding to filter our data to focus on regions with significant coverage. While there were around 300,000 bins across the three chromosomes we looked at, after thresholding our data consisted of roughly 10% or 30,000 bins. Our data does not contain any coverage values below the threshold. In the pitfalls section, we will explore how a model performs with and without regions of zero signal. Methods for dealing with class imbalances: Scikit-learn’s ‘imbalance-learn’ package (Oversampling, Undersampling and Weighting) “Imbalanced-learn (imported as imblearn) is an open source, MIT-licensed library relying on scikit-learn (imported as sklearn) and provides tools when dealing with classification with imbalanced classes.” SMOTE (Oversampling) “a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.” ADASYN (Oversampling) “similar to SMOTE but it generates different number of samples depending on an estimate of the local distribution of the class to be oversampled.” References Retel, Joren Sebastian, Andreas Poehlmann, Josh Chiou, Andreas Steffen, and Djork-Arné Clevert. 2024. “A Fast Machine Learning Dataloader for Epigenetic Tracks from BigWig Files.” Bioinformatics 40 (1): btad767. https://doi.org/10.1093/bioinformatics/btad767. Whalen, Sean, Jacob Schreiber, William Noble, and Katherine Pollard. 2022. “Navigating the Pitfalls of Applying Machine Learning in Genomics.” https://www.nature.com/articles/s41576-021-00434-9. Retel et al. (2024)↩︎ Whalen et al. (2022)↩︎ "],["loss-functions-and-peak-metrics.html", "3 Loss functions, and peak metrics", " 3 Loss functions, and peak metrics When selecting the optimal loss function for your machine learning model in genomics, the decision should be informed by the nature of the problem and the specific type of data you’re working with. The principles for choosing loss functions in genomics are similar to those in other machine learning contexts. For regression based tasks, while Mean Squared Error (mse) loss functions have been used, models utilising data and making predictions associated with reads or counts use a Poisson loss function. Another two common loss functions include Binary Cross-Entropy loss and Categorical Cross-Entropy loss, when dealing with classification type predictions.19 Mean Squared Error (MSE): • Use Case: Regression problems where the goal is to predict continuous values, such as gene expression or coverage levels. Example: DeepImpute, a deep neural network-based imputation algorithm that allows for accurate imputation of single-cell RNA-seq data. The model is used to estimate missing or low-quality gene expression values in single-cell RNA sequencing datasets. It uses a “weighted mean squared error (MSE) loss function that gives higher weights to genes with higher expression values. This emphasizes accuracy on high confidence values and avoids over penalizing genes with extremely low values (e.g., zeros)”.20 Poisson Loss: • Use Case: Count data where the number of events (e.g., read counts in chIP-seq data) follows a Poisson distribution. Example: A deep learning architecture called Enformer was used to predict gene expression more accurately in 2021 by integrating long range interactions within the genome. It utilises a poisson negative log-likelihood loss function resulting in a model that was able to integrate information from up to 100 kilobases away.21 Cross-Entropy Loss: • Use Case: Classification problems where the goal is to predict binary outcomes. Example: A convolutional neural network was employed to create a software pipeline called CNN-Peaks, designed to categorically detect ChIP-Seq peaks without relying on traditional peak calling methods or manual inspection. The model utilizes a binary cross-entropy loss function, which was weighted to account for the scarcity of peaks in the data.22 Categorical Cross-Entropy Loss: • Use Case: Multi-class classification problems. Example: Researchers developed a combined model that integrates cell-free DNA (cfDNA) methylation profile data with ATAC-seq data to enhance cancer detection and tissue-of-origin localization. This approach combines both epigenomic and chromatin accessibility information to improve the accuracy of identifying the specific tissue or organ from which a cancerous signal originates. The model employs a categorical cross-entropy loss function within each component to optimize tissue-of-origin localization, allowing it to effectively determine the most likely source of the cancerous signal.23 In the case of this tutorial and running models to predict continuous coverage values from bigwig p-value datasets, I have opted to use a Poisson loss function as the data represents read counts. It is important to remember that “loss functions can penalize the shapes or the magnitudes (for example, the mean squared error (MSE))”24 when optimising. References Arisdakessian, Cédric, Olivier Poirion, Breck Yunits, Xun Zhu, and Lana Garmire. 2019. DeepImpute: An Accurate, Fast, and Scalable Deep Neural Network Method to Impute Single-Cell RNA-Seq Data. Springer Nature. https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1837-6. Avsec, Žiga, Vikram Agarwal, Daniel Visentin, and et al. 2021. Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions. Springer Nature. https://www.nature.com/articles/s41592-021-01252-x. Bae, Mingyun, Gyuhee Kim, Tae-Rim Lee, and et al. 2017. Integrative Modeling of Tumor Genomes and Epigenomes for Enhanced Cancer Diagnosis by Cell-Free DNA. Nature Communications. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10085982/. Oh, Dongpin, Seth Strattan, Junho Hur, and et al. 2020. CNN-Peaks: ChIP-Seq Peak Detection Pipeline Using Convolutional Neural Networks That Imitate Human Visual Inspection. Springer Nature. https://www.nature.com/articles/s41598-020-64655-4. Patterson, Josh, and Adam Gibson. 2017. Deep Learning: A Practitioner’s Approach. O’Reilly Media, Inc. https://www.oreilly.com/library/view/deep-learning/9781491924570/. Toneyan, Shushan, Ziqi Tang, and Peter Koo. 2022. Evaluating Deep Learning for Predicting Epigenomic Profiles. Springer Nature. https://www.nature.com/articles/s42256-022-00570-9. Patterson and Gibson (2017)↩︎ Arisdakessian et al. (2019)↩︎ Avsec et al. (2021)↩︎ Oh et al. (2020)↩︎ Bae et al. (2017)↩︎ Toneyan, Tang, and Koo (2022)↩︎ "],["training-tricks.html", "4 Training tricks 4.1 Reverse Complements and Sequence Shifts 4.2 Hyper-parameter optimisation", " 4 Training tricks 4.1 Reverse Complements and Sequence Shifts Reverse Complements As explained in Part 1, DNA has a double helix structure. When we one-hot encode a segment of DNA in our models using a reference genome, we typically represent only one strand of the double helix. The complement of this strand is the opposite strand, where Adenine (A) pairs with Thymine (T), and Cytosine (C) pairs with Guanine (G). The reverse complement of a DNA strand is obtained by first taking its complement and then reading it in the reverse direction. This figure shows the reverse complement of a DNA sequence25 Training on DNA sequences and augmenting the data with their reverse complements has been shown to improve model accuracy, prediction, and interpretability in DNA sequence-related models. This approach involves “treating the reverse complement DNA sequence as another sample” (Cao and Zhang 2019). By incorporating reverse complements, the model is exposed to a wider variety of sequence patterns, which helps reduce overfitting and enhances generalization. As a result, models become better at recognizing patterns regardless of strand orientation. Although the logic to obtain the reverse complement of a DNA strand is straightforward, the Bio.Seq module from the Biopython library provides a simple way to do this. Augmenting your dataset with reverse complements is usually done to training sets, but can be applied to validation and test sets as well. from Bio.Seq import Seq # Example DNA sequence dna_sequence = Seq(&quot;ATGCGTAC&quot;) # Generate the reverse complement reverse_complement = dna_sequence.reverse_complement() print(&quot;Original Sequence: &quot;, dna_sequence) ## Original Sequence: ATGCGTAC print(&quot;Reverse Complement: &quot;, reverse_complement) ## Reverse Complement: GTACGCAT Sequence Shifts Training on small, random sequence shifts up and downstream by shifting the genomic coordinates of the input sequence is also known as jitter. Jittering adds diversity to the training data by creating slightly different versions of the same sequence. This allows models to be less sensitive to the exact positioning of features, making them more robust to variations in the data. It also allows models to generalise better to unseen data where sites may not always be perfectly aligned. A variation of jittering, called flanking “extends DNA sequences from its midpoint by X base pairs and takes the left, middle and right input windows of the extended sequence as training samples with the same labels, tripling the size of training set.26 Implementing data augmentations using reverse complements and sequence shifts can be approached in different ways. Similar to the ‘flanking’ example, you can either expand your dataset by adding additional augmented data points or apply a random augmentation strategy, where only some data points are randomly augmented while keeping the total number of points in your dataset unchanged. Data augmentations are usually applied only to training sets, however in the context of computer vision “many research reports have shown the effectiveness of augmenting data at test-time as well”.27 When implementing augmentations like reverse complements and sequence shifts, these are typically applied after splitting your data into training, validation, and test sets. When applying sequence shifting, it’s logical to shift the interval before retrieving the nucleotide sequence from the reference genome. The reverse complement should be applied after retrieving the nucleotide sequence but before one-hot encoding it. If you’re using the BioPython library, this works well since BioPython’s reverse complement function operates on string inputs. In the genomics context, a paper on evaluating deep learning for predicting epigenomic profiles used two convolutional neural networks, Basenji and BPNet, trained on ATAC-seq data, to predict coverage values as a regression. They found that convolutional models trained with augmentations (reverse complement and sequence shifts), “yielded improved robustness, especially when trained on peak-centered data (BPNet). On the other hand, models that were trained on coverage-threshold data (Basenji) already benefited from the randomly-centered profiles.”28 Additionally, while they initially “used a MSE and multinomial NLL loss for BPNet, [they] found that optimization using Poisson NLL yielded better performance.”29 This finding is another motivation of using at poisson loss function in subsequent tutorials. 4.2 Hyper-parameter optimisation Which learning rates are commonly used? How many epochs are typically used to train on? While the learning rate and number of epochs differ by model and study, based on some of the research cited so far, common learning rates are in the range 1e-430 to 1e-331. Additionally, some studies apply learning rate decay if the loss function shows no improvement over time32 while others lower the learning rate for fine tuning.33 The number of epochs used to train on differs by quite a margin. In training a convolution neural network to explore the effects of genomic data augmentation, 30 epochs were used.34 DeepImpute which constructs multiple sub-neural networks for genotype imputation, trains on a maximum of 500 epochs, while the study involving the Basenji and BPNet models were trained on a maximum of 100 epochs. The clear strategy for these larger models involve the use of early stopping if no improvements are evident after 5-10 epochs. When hyperparameter optimising, the consensus for achieving the best model performance is to train with a high number of epochs to enable the model to confidently learn features as they apply to labels, starting with a high learning rate, and decreasing over time using a learning rate scheduler. Interestingly, a study on binary peak detection using CNNs on ChIP-Seq data manually tuned their model’s hyperparameters and found that little changes in performance results35. This highlights the challenges of hyperparameter tuning with larger models, where manually fine-tuning is not ideal. How can hyperparamter tuning on these larger models be done in practice? Raytune Raytune “is a Python library for experiment execution and hyperparameter tuning at any scale”. It aids in leveraging state of the art hyperparameter optmisation algorithms while simplifying scaling for larger models. Raytune hyperparameter searching can also be scaled to cloud based clusters without the need for large changes in code structure. Additionally, it supports several machine learning frameworks such as pyTorch and TensorFlow36. One of the strongest current hyperparameter optmisation algorithms is the Asynchronous Successive Halving Algorithm or ASHA. Asha “exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems”37, allowing for faster optimisation and applicability to the larger models common in genomics. A study on predicting the impact of sequence motifs on gene regulation utilised Raytune and the ASHA algorithm to successfully optimise their model’s hyperparameters38. In the genomic context, as a result of complex models using large genomic datasets, hyperparameter tuning using a brute force approach is untenable. Utilising existing libraries such as Raytune and incorporating asynchronous algorithms such as ASHA, has the potential to pave the way forward in improving model performance without unreasonable computational costs. References Arisdakessian, Cédric, Olivier Poirion, Breck Yunits, Xun Zhu, and Lana Garmire. 2019. DeepImpute: An Accurate, Fast, and Scalable Deep Neural Network Method to Impute Single-Cell RNA-Seq Data. Springer Nature. https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1837-6. Avsec, Žiga, Vikram Agarwal, Daniel Visentin, and et al. 2021. Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions. Springer Nature. https://www.nature.com/articles/s41592-021-01252-x. Cao, Zhen, and Shihua Zhang. 2019. Simple Tricks of Convolutional Neural Network Architectures Improve DNA–Protein Binding Prediction. Bioinformatics. https://academic.oup.com/bioinformatics/article/35/11/1837/5142724?login=false. Hepkema, Jacob, Nicholas Lee, Benjamin Stewart, and et al. 2023. Predicting the Impact of Sequence Motifs on Gene Regulation Using Single-Cell Data. National Library of Medicine. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10426127/. Li, Liam. 2018. A System for Massively Parallel Hyperparameter Tuning. Conference on Machine Learning; Systems. https://arxiv.org/abs/1810.05934. Liaw, Richard, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez, and Ion Stoica. 2018. “Tune: A Research Platform for Distributed Model Selection and Training.” arXiv Preprint arXiv:1807.05118. Oh, Dongpin, Seth Strattan, Junho Hur, and et al. 2020. CNN-Peaks: ChIP-Seq Peak Detection Pipeline Using Convolutional Neural Networks That Imitate Human Visual Inspection. Springer Nature. https://www.nature.com/articles/s41598-020-64655-4. Shorten, Connor, and Taghi Khoshgoftaar. 2019. Design Considerations for Image Data Augmentation. Journal of Big Data. https://link.springer.com/article/10.1186/s40537-019-0197-0? Toneyan, Shushan, Ziqi Tang, and Peter Koo. 2022. Evaluating Deep Learning for Predicting Epigenomic Profiles. Springer Nature. https://www.nature.com/articles/s42256-022-00570-9. Youens-Clark, Ken. 2021. Mastering Python for Bioinformatics. O’Reily Media. https://www.oreilly.com/library/view/mastering-python-for/9781098100872/ch03.html. Youens-Clark (2021)↩︎ Cao and Zhang (2019)↩︎ Shorten and Khoshgoftaar (2019)↩︎ Toneyan, Tang, and Koo (2022)↩︎ Toneyan, Tang, and Koo (2022)↩︎ (Arisdakessian et al. 2019), (Avsec et al. 2021)↩︎ (Cao and Zhang 2019), (Toneyan, Tang, and Koo 2022)↩︎ Toneyan, Tang, and Koo (2022)↩︎ Avsec et al. (2021)↩︎ Cao and Zhang (2019)↩︎ Oh et al. (2020)↩︎ Liaw et al. (2018)↩︎ Li (2018)↩︎ Hepkema et al. (2023)↩︎ "],["reproducibility-of-machine-learning-models.html", "5 Reproducibility of machine learning models", " 5 Reproducibility of machine learning models The ability to reproduce machine learning models and results is of extreme importance in the genomic context. According to an article on reproducibility standards for machine learning in the life sciences, the bronze standard for reproducibility includes access to the original data, trained model and source code. The silver standard incorporates computational environment considerations as well as inherent non-determinism, while the gold standard requires automation such that the experiment is reproducible with a single command.39 Seeding Seeding runs and setting random number seeds aids in eliminating the inherent non-determinism of creating data splits and training models. # Seeding to ensure reproducibility import random import numpy as np import torch seed = 42 # or any other integer random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) #CPU torch.cuda.manual_seed(seed) #GPU torch.backends.cudnn.deterministic = True # To ensure deterministic behavior torch.backends.cudnn.benchmark = False # Ensures reproducibility This seeding example shows a seed being used for python’s built in random library, numpy’s random library, and the PyTorch library for both CPUs and GPUs. Additionally, PyTorch’s CuDNN library accelerates computations on GPUs. By setting CuDNN to deterministic mode and disabling benchmarking, you ensure that the same algorithm is used in each run. This approach reduces variability due to hardware differences at the cost of computational efficiency. Dashboarding Dashboarding can be used to keep track of model performance as well as model and training parameters by visualising logged data. Dashboards such as Weights and Biases (WandB) can be leveraged to keep track of model runs, hyperparameters and even trained models and data through uploading them as artifacts. Source: WandB Real time updates on metrics are important in the genomic context when models can take extended periods of time to train. Furthermore, downloading and sharing specific models, additional artifacts and training logs with information on environment settings becomes easier when utilising dashboards. While not explicitly ensuring or enabling reproducibility, dashboarding helps provide transparency. References Heil, Benjamin, Michael Hoffman, Florian Markowetz, and et al. 2021. Reproducibility Standards for Machine Learning in the Life Sciences. https://www.nature.com/articles/s41592-021-01256-7. Heil et al. (2021)↩︎ "],["testing.html", "6 Testing", " 6 Testing Testing and evaluating genomic models is similar to other machine learning contexts in that it involves assessing model performance using metrics such as accuracy, precision, recall, and F1-scores. However, the genomic context often requires additional considerations due to the complexity and high-dimensionality of genomic data. There are several pitfalls which when fallen into, inflate model performance and make test metrics untrustworthy. Genomic data can be extremely imbalanced, as we will find out in subsequent tutorials. In these unbalanced classes scenarios, the choice of test metric usually between the area under the receiver operating characteristic (auROC) and the area under the precision-recall curve (auPRC), as only predicting the majority class will result in high accuracy for categorical models. Source: BlogPost In genomic situations with extreme imbalance in data, plotting ROC and precision-recall curves and calculating the auROC and auPRC is beneficial to properly test your model’s performance. AuROC and auPRC both provide insight into the model’s ability to distinguish between classes and are both functions of recall (true positive rate). “In many genomics problems, high recall can be achieved at a very low [false positive rate] owing to the large number of negatives in the test set, making it easy to obtain a high auROC even when false positives vastly outnumber true positives”40. However, auPRC offers a more focused evaluation as the absolute number of false positives is taken into account in precision versus the false positive rate. Therefore, a model trained on imbalanced data that predicts many false positives could have a high auROC and a low auPRC. In cases where we are interested in a minority of positive classes, research suggests using the auPRC as a measure of performance41. In regression settings, classic metrics such as the mean squared error, and mean absolute error are used alongside measures of variance (\\(R^2\\)) and correlation (pearson correlation). It is important to note that differences in the order of magnitude of multiple sets of predictions can affect MSE metrics when evaluated together. For example, if a multi-task model has two prediction sets where one set “has values entirely between 1,000 and 10,000 and the other [set] has values between 0.01 and 0.1, then evaluating the model simply using mean squared error (MSE) across the two tasks will largely ignore the second task.”42 In the genomic context one can also perform marginalisation experiments to ensure models are recognizing biologically relevant patterns. “Marginalisation is a method that requires summing over the possible values of one variable to determine the marginal contribution of another”43. Given a model trained on DNA sequences, performing a marginalisation experiment using sequence motifs is possible. “Motifs are short, recurring patterns in DNA that are presumed to have a biological function”44. Marginalising with motifs involves comparing a model’s predictions before and after the motif is inserted into a test/prediction set. The difference in predictions allows us to quantify the influence of a motif on a model’s predictions and understand how well our model recognises biological patterns. More specific types of marginalisation experiments include variant effect prediction, which aims to understand the effect of specific genetic variants in sequences on a model’s predictions. References Brooks-Bartlett, Jonny. 2018. Probability Concepts Explained: Marginalisation. Towards Data Science. https://towardsdatascience.com/probability-concepts-explained-marginalisation-2296846344fc. D’haeseleer, Patrik. 2006. What Are DNA Sequence Motifs? Nat Biotechnol. https://www.nature.com/articles/nbt0406-423. Whalen, Sean, Jacob Schreiber, William Noble, and Katherine Pollard. 2022. “Navigating the Pitfalls of Applying Machine Learning in Genomics.” https://www.nature.com/articles/s41576-021-00434-9. Whalen et al. (2022)↩︎ Whalen et al. (2022)↩︎ Whalen et al. (2022)↩︎ Brooks-Bartlett (2018)↩︎ D’haeseleer (2006)↩︎ "],["software-libraries-for-model-building.html", "7 Software libraries for model building 7.1 gReLU 7.2 Kipoi", " 7 Software libraries for model building 7.1 gReLU gReLU is a Python library to train, interpret, and apply deep learning models to DNA sequences”. The gRelu library contains a model zoo allowing for easy access to several models such as Borzoi, Enformer, or a dilated convolutional model based on the BPnet model architecture. Some of these models can be imported pre-trained. Additionally, simpler base models and convolutional neural networks are also available. On top of access to already built models, the software library allows for designing your own models. Source: gRelu Their documentation contains all their available functions and includes tutorials on using gRelu. 7.2 Kipoi Kipoi is a repository of ready-to-use trained models for genomics. Referring back to the reproducibility of machine learning models, Kipoi contains 2206 different gold standard models, available to be downloaded and tested in a few lines of code. Source: Kipoi Similarly to gRelu, they include several use case tutorials and a model zoo. Downloaded models can also be built upon to conduct further research as links to the github source code of each model are provided. "],["pitfalls-overview.html", "8 Pitfalls overview", " 8 Pitfalls overview Applying machine learning in the field of genomics comes with its own challenges. Genomic data is highly complex, featuring high dimensionality, heterogeneity, and noise. It is important to consider the assumptions models make and whether those assumptions hold true within the data. According to research, available here, on pitfalls in machine learning, the most common errors include not taking into account distributional differences between contexts or batches, dependencies within the data, confounding effects distorting true relationships, unbalanced classes and leaking information between datasets.45 Lets examine the pitfalls mentioned in the research above and methods to mitigate these effects in more detail. Source: Nature Reviews Genetics Distributional differences Distributional differences between the context in which a model was trained and tested, and the context in which a model makes predictions, results in different model performances. These different contexts can be attributed to batch effects or applying the model on different cell types. “One should expect that performance will be higher in [the same setting] than on [different settings]46. It is crucial then, for models which aim to predict biological relationships across contexts to be validated and tested across these different settings that accurately reflect the real-world variability. Even models that aim to predict across one context such as cell type-specific models common in disease prediction, are still susceptible to this pitfall. When using data from multiple experiments, batch effects can introduce variability that lead to differences between training, test, and prediction sets. To address this, it is important to consider batch effects during model development and evaluation. One approach is to ensure that training and test sets are sourced from different batches to evaluate the generalisability of the model and prevent inflated performance. Additionally, applying batch effect correction techniques such can help account for these distributional differences. Dependent examples When studying machine learning, a constant assumption is that of independence. Whether it be independent test sets or data points being independent of each other, the assumption of independence is key in creating fair models. Many biological processes in genomics are not independent.47 A research paper focused on understanding the pitfalls of neural networks predicting across cell types found that several models had been evaluated on test sets comprising of cell type independent of training, but not independent of chromosomes leading to inflated performances.48 That is, gene expression predictions were evaluated on different cells but on the same chromosomes. The inflated performances were due to chromosomes “themselves [being] dependent across samples because the underlying functional activity is generally shared”49. While hard to identify in the designing phase even through visualisation, mitigation techniques include preventing overfitting during evaluation and group k-fold cross-validation to prevent leakage between training and test sets.50 Confounding Confounders are additional variables that affect the variables being studied, resulting in models not capturing the correct relationships in the data.51 “Confounding in genetic studies can arise from unmodelled environmental factors and population structure, as well as other factors”.52 In the context of ATAC-Seq and chIP-Seq data, differences in the sequencing depth of the experiment can have a confounding effect on models if not handled. As explained in part 1, ATAC-Seq and chIP-Seq data consists of reads that are alligned to a reference genome and aggregated. The sequencing depth of the experiment refers to on average how many times a region was sequenced.53 A higher sequencing depth means a higher overall coverage level. If not accounted for, models using data of different sequencing depths are confounded. Similarly when predicting on a dataset with a different sequencing depth than the training set, the model would biased. While sequencing depth as a confounder can be more easily corrected by downsampling the raw reads from the dataset with higher sequencing depth, other confounding effects can be harder to account for. However, it is possible to use statistical tools such as PEER54, Inter-sample Correlation Emended (ICE) and Surrogate Variable Analysis (SVA)55 to understand confounders in your dataset. Leaky pre-processing Leaky pre-processing involves information leaks between the training and test sets resulting in altered testing metrics. In genomics, as a result of many dependencies, pre-processing steps involving the dataset as a whole can introduce this bias. This includes standardisation techniques, supervised feature selection or principle component analysis, before splitting data into test splits.56 A solution to this is to perform these transformations and analysis after data splits, preferably within cross-validation. Leaky preprocessing has been prevalent in various genomic fields, including microarray analysis, DNA methylation, gene expression studies, and more, often leading to misleading results.57 Unbalanced classes Unbalanced datasets can be common in genomics often necessitating the use of statistical methods to validate the few positive instances amid vast amounts of data. In these cases, the pitfall is a model “learns most of the target concepts of the majority class and learns target concepts from the minority class poorly or not at all.”58 This is particularly evident in disease related tasks where the focus is on a few disease causing genes or non-coding variants.59 Additionally, many experiments that implement conservative significance thresholds to determine true signals involve data imbalance, such as peak detection60, gene expression61, and chromatin accessibility. The extensive size of the human genome exacerbates this issue when the areas of interest are small. Researchers address this imbalance by employing balancing algorithms that oversample the negative class and undersample the majority class. For instance, in training models to predict functional peaks from ChIP-seq or chromatin accessibility data, an approach might involve using all identified peaks along with a matching number of negative regions, thus effectively undersampling the majority class. For datasets with no such negative regions, researchers have to construct their own. While such imbalances are commonly discussed in classification, they also pose challenges in regression models predicting quantitative outcomes, where performance may be compromised in regions with sparse data, such as genomic areas or genes with low read counts in single-cell genomics studies.62 References Allis, D., Jenuwein T, Reinberg D, and Caparros M. L. 2015. EPIGENETICS. 2nd ed. Cold Spring Harbor Laboratory Press. https://www.cshlpress.com/pdf/sample/2014/epigenetics2/EPIFM.pdf. Avsec, Žiga, Vikram Agarwal, Daniel Visentin, and et al. 2021. Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions. Springer Nature. https://www.nature.com/articles/s41592-021-01252-x. Haque, Muksitul, and et al. 2014. Imbalanced Class Learning in Epigenetics. Journal of Computational Biology. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4082351/. Listgarten, Jennifer, and et al. 2010. Correction for Hidden Confounders in the Genetic Analysis of Gene Expression. Proceedings of the National Academy of Sciences of the United States of America. https://www.pnas.org/doi/full/10.1073/pnas.1002425107. Oh, Dongpin, Seth Strattan, Junho Hur, and et al. 2020. CNN-Peaks: ChIP-Seq Peak Detection Pipeline Using Convolutional Neural Networks That Imitate Human Visual Inspection. Springer Nature. https://www.nature.com/articles/s41598-020-64655-4. Pourhoseingholi, Mohamad, and et al. 2012. How to Control Confounding Effects by Statistical Analysis. Gastroenterol Hepatol Bed Bench. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4017459/#:~:text=A%20Confounder%20is%20an%20extraneous,between%20the%20variables%20under%20study. Schreiber, Jacob, and et al. 2020. A Pitfall for Machine Learning Methods Aiming to Predict Across Cell Types. Springer Nature. https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02177-y. Sims, David, and et al. 2014. Sequencing Depth and Coverage: Key Considerations in Genomic Analyses. Nature Reviews Genetics. https://www.nature.com/articles/nrg3642. Stegle, Oliver, and et al. 2012. Using Probabilistic Estimation of Expression Residuals (PEER) to Obtain Increased Power and Interpretability of Gene Expression Analyses. Nature Protocols. https://www.nature.com/articles/nprot.2011.457. Whalen, Sean, Jacob Schreiber, William Noble, and Katherine Pollard. 2022. “Navigating the Pitfalls of Applying Machine Learning in Genomics.” https://www.nature.com/articles/s41576-021-00434-9. Whalen et al. (2022)↩︎ Whalen et al. (2022)↩︎ Allis et al. (2015)↩︎ Schreiber and al (2020)↩︎ Whalen et al. (2022)↩︎ Whalen et al. (2022)↩︎ Pourhoseingholi and al (2012)↩︎ Whalen et al. (2022)↩︎ Sims and al (2014)↩︎ Stegle and al (2012)↩︎ Listgarten and al (2010)↩︎ Whalen et al. (2022)↩︎ Whalen et al. (2022)↩︎ Haque and al (2014)↩︎ Haque and al (2014)↩︎ Oh et al. (2020)↩︎ Avsec et al. (2021)↩︎ Whalen et al. (2022)↩︎ "],["using-grelu-models.html", "9 Using gReLU models", " 9 Using gReLU models Continuing from previous tutorials, the next tutorial similarly uses chIP-Seq data from Encode Experiment ENCSR817LUF trained on a gRelu model in order to to predict the coverage levels and examine several of the pitfalls mentioned in section 4. An additional tutorial is included to explain how data was pre-processed for gRelu to explore Tutorial 1.5: Preprocessing Data for gRelu Models (interactive) Tutorial 1.5: Preprocessing Data for gRelu Models (nbviewer) Tutorial 2: Training Models with gRelu and Examining Pitfalls (interactive) Tutorial 2: Training Models with gRelu and Examining Pitfalls (nbviewer) "],["references.html", "References", " References Akalin, Altuna. 2020. Computational Genomics with r. Github Pages. https://compgenomr.github.io/book/. Al-Aboud, Nora M., Connor Tupper, and Ishwarlal Jialal. 2023. Genetics, Epigenetic Mechanism. National Library of Medicine. https://www.ncbi.nlm.nih.gov/books/NBK532999/#article-22137.r1. Allis, D., Jenuwein T, Reinberg D, and Caparros M. L. 2015. EPIGENETICS. 2nd ed. Cold Spring Harbor Laboratory Press. https://www.cshlpress.com/pdf/sample/2014/epigenetics2/EPIFM.pdf. Arisdakessian, Cédric, Olivier Poirion, Breck Yunits, Xun Zhu, and Lana Garmire. 2019. DeepImpute: An Accurate, Fast, and Scalable Deep Neural Network Method to Impute Single-Cell RNA-Seq Data. Springer Nature. https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1837-6. Avsec, Žiga, Vikram Agarwal, Daniel Visentin, and et al. 2021. Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions. Springer Nature. https://www.nature.com/articles/s41592-021-01252-x. Bae, Mingyun, Gyuhee Kim, Tae-Rim Lee, and et al. 2017. Integrative Modeling of Tumor Genomes and Epigenomes for Enhanced Cancer Diagnosis by Cell-Free DNA. Nature Communications. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10085982/. Board, PDQ® Cancer Genetics Editorial. n.d. Genetics-Dictionary. National Cancer Institute USA. https://www.cancer.gov/publications/dictionaries/genetics-dictionary. Brooks-Bartlett, Jonny. 2018. Probability Concepts Explained: Marginalisation. Towards Data Science. https://towardsdatascience.com/probability-concepts-explained-marginalisation-2296846344fc. Cao, Zhen, and Shihua Zhang. 2019. Simple Tricks of Convolutional Neural Network Architectures Improve DNA–Protein Binding Prediction. Bioinformatics. https://academic.oup.com/bioinformatics/article/35/11/1837/5142724?login=false. D’haeseleer, Patrik. 2006. What Are DNA Sequence Motifs? Nat Biotechnol. https://www.nature.com/articles/nbt0406-423. Green, Phil, and Brent Ewing. n.d. Phred - Quality Base Calling. CodonCode Corporation. https://www.phrap.com/phred/#:~:text=Phred%20is%20a%20base%2Dcalling,%22)%20to%20each%20base%20call. Haque, Muksitul, and et al. 2014. Imbalanced Class Learning in Epigenetics. Journal of Computational Biology. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4082351/. Heil, Benjamin, Michael Hoffman, Florian Markowetz, and et al. 2021. Reproducibility Standards for Machine Learning in the Life Sciences. https://www.nature.com/articles/s41592-021-01256-7. Hepkema, Jacob, Nicholas Lee, Benjamin Stewart, and et al. 2023. Predicting the Impact of Sequence Motifs on Gene Regulation Using Single-Cell Data. National Library of Medicine. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10426127/. Kanani, Pratik, and Mamta Padole. 2020. Improving Pattern Matching Performance in Genome Sequences Using Run Length Encoding in Distributed Raspberry Pi Clustering Environment. Procedia Computer Science. https://www.sciencedirect.com/science/article/pii/S1877050920311601?via%3Dihub. Kappelmann-Fenzl, Melanie. 2021. Design and Analysis of Epigenetics and ChIP-Sequencing Data. Springer. https://doi.org/10.1007/978-3-030-62490-3_12. Li, Liam. 2018. A System for Massively Parallel Hyperparameter Tuning. Conference on Machine Learning; Systems. https://arxiv.org/abs/1810.05934. Liaw, Richard, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez, and Ion Stoica. 2018. “Tune: A Research Platform for Distributed Model Selection and Training.” arXiv Preprint arXiv:1807.05118. Listgarten, Jennifer, and et al. 2010. Correction for Hidden Confounders in the Genetic Analysis of Gene Expression. Proceedings of the National Academy of Sciences of the United States of America. https://www.pnas.org/doi/full/10.1073/pnas.1002425107. Mistry, Meeta, Jihe Liu, Radhika Khetani, and et al. 2022. Peak Calling with MACS2. Github Pages. https://hbctraining.github.io/Intro-to-ChIPseq-flipped/lessons/06_peak_calling_macs.html. Oh, Dongpin, Seth Strattan, Junho Hur, and et al. 2020. CNN-Peaks: ChIP-Seq Peak Detection Pipeline Using Convolutional Neural Networks That Imitate Human Visual Inspection. Springer Nature. https://www.nature.com/articles/s41598-020-64655-4. Patel, Jai. 2024. Leveraging Genetic Diversity with Machine Learning. Imperial College London. Patterson, Josh, and Adam Gibson. 2017. Deep Learning: A Practitioner’s Approach. O’Reilly Media, Inc. https://www.oreilly.com/library/view/deep-learning/9781491924570/. Pourhoseingholi, Mohamad, and et al. 2012. How to Control Confounding Effects by Statistical Analysis. Gastroenterol Hepatol Bed Bench. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4017459/#:~:text=A%20Confounder%20is%20an%20extraneous,between%20the%20variables%20under%20study. Ralston, Amy, and Kenna Shaw. 2008. Gene Expression Regulates Cell Differentiation. Nature Education. https://www.nature.com/scitable/topicpage/gene-expression-regulates-cell-differentiation-931/#:~:text=All%20of%20the%20cells%20within,each%20cell%20deploys%20its%20genome. Retel, Joren Sebastian, Andreas Poehlmann, Josh Chiou, Andreas Steffen, and Djork-Arné Clevert. 2024. “A Fast Machine Learning Dataloader for Epigenetic Tracks from BigWig Files.” Bioinformatics 40 (1): btad767. https://doi.org/10.1093/bioinformatics/btad767. Schreiber, Jacob, and et al. 2020. A Pitfall for Machine Learning Methods Aiming to Predict Across Cell Types. Springer Nature. https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02177-y. Shahid, Zainab, Brittany Simpson, Kathleen H. Miao, and Gurdeep Singh. 2023. Genetics, Histone Code. StatPearls Publishing LLC. https://www.ncbi.nlm.nih.gov/books/NBK538477/. Shorten, Connor, and Taghi Khoshgoftaar. 2019. Design Considerations for Image Data Augmentation. Journal of Big Data. https://link.springer.com/article/10.1186/s40537-019-0197-0? Sims, David, and et al. 2014. Sequencing Depth and Coverage: Key Considerations in Genomic Analyses. Nature Reviews Genetics. https://www.nature.com/articles/nrg3642. Stegle, Oliver, and et al. 2012. Using Probabilistic Estimation of Expression Residuals (PEER) to Obtain Increased Power and Interpretability of Gene Expression Analyses. Nature Protocols. https://www.nature.com/articles/nprot.2011.457. T., Kouzarides. 2007. Chromatin Modifications and Their Function. National Library of Medicine. https://doi.org/10.1016/j.cell.2007.02.005. Toneyan, Shushan, Ziqi Tang, and Peter Koo. 2022. Evaluating Deep Learning for Predicting Epigenomic Profiles. Springer Nature. https://www.nature.com/articles/s42256-022-00570-9. Whalen, Sean, Jacob Schreiber, William Noble, and Katherine Pollard. 2022. “Navigating the Pitfalls of Applying Machine Learning in Genomics.” https://www.nature.com/articles/s41576-021-00434-9. Wilbanks, Elizabeth, and Marc Facciotti. 2010. Evaluation of Algorithm Performance in ChIP-Seq Peak Detection. Plos One Journal. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0011471. Youens-Clark, Ken. 2021. Mastering Python for Bioinformatics. O’Reily Media. https://www.oreilly.com/library/view/mastering-python-for/9781098100872/ch03.html. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
