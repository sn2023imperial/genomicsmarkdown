[["loss-functions-and-peak-metrics.html", "3 Loss functions, and peak metrics", " 3 Loss functions, and peak metrics When selecting the optimal loss function for your machine learning model in genomics, the decision should be informed by the nature of the problem and the specific type of data you’re working with. The principles for choosing loss functions in genomics are similar to those in other machine learning contexts. For regression based tasks, while Mean Squared Error (mse) loss functions have been used, models utilising data and making predictions associated with reads or counts use a Poisson loss function. Another two common loss functions include Binary Cross-Entropy loss and Categorical Cross-Entropy loss, when dealing with classification type predictions.1 Mean Squared Error (MSE): • Use Case: Regression problems where the goal is to predict continuous values, such as gene expression or coverage levels. Example: DeepImpute, a deep neural network-based imputation algorithm that allows for accurate imputation of single-cell RNA-seq data. The model is used to estimate missing or low-quality gene expression values in single-cell RNA sequencing datasets. It uses a “weighted mean squared error (MSE) loss function that gives higher weights to genes with higher expression values. This emphasizes accuracy on high confidence values and avoids over penalizing genes with extremely low values (e.g., zeros)”.2 Poisson Loss: • Use Case: Count data where the number of events (e.g., read counts in chIP-seq data) follows a Poisson distribution. Example: A deep learning architecture called Enformer was used to predict gene expression more accurately in 2021 by integrating long range interactions within the genome. It utilises a poisson negative log-likelihood loss function resulting in a model that was able to integrate information from up to 100 kilobases away.3 Cross-Entropy Loss: • Use Case: Classification problems where the goal is to predict binary outcomes. Example: A convolutional neural network was employed to create a software pipeline called CNN-Peaks, designed to categorically detect ChIP-Seq peaks without relying on traditional peak calling methods or manual inspection. The model utilizes a binary cross-entropy loss function, which was weighted to account for the scarcity of peaks in the data.4 Categorical Cross-Entropy Loss: • Use Case: Multi-class classification problems. Example: Researchers developed a combined model that integrates cell-free DNA (cfDNA) methylation profile data with ATAC-seq data to enhance cancer detection and tissue-of-origin localization. This approach combines both epigenomic and chromatin accessibility information to improve the accuracy of identifying the specific tissue or organ from which a cancerous signal originates. The model employs a categorical cross-entropy loss function within each component to optimize tissue-of-origin localization, allowing it to effectively determine the most likely source of the cancerous signal.5 In the case of this tutorial and running models to predict continuous coverage values from bigwig p-value datasets, I have opted to use an MSE loss function, however I also compare its results to a model with a Poisson loss function. It is important to remember that “loss functions can penalize the shapes or the magnitudes (for example, the mean squared error (MSE))”6 when optimising. References Arisdakessian, Cédric, Olivier Poirion, Breck Yunits, Xun Zhu, and Lana Garmire. 2019. DeepImpute: An Accurate, Fast, and Scalable Deep Neural Network Method to Impute Single-Cell RNA-Seq Data. Springer Nature. https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1837-6. Avsec, Žiga, Vikram Agarwal, Daniel Visentin, and et al. 2021. Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions. Springer Nature. https://www.nature.com/articles/s41592-021-01252-x. Bae, Mingyun, Gyuhee Kim, Tae-Rim Lee, and et al. 2017. Integrative Modeling of Tumor Genomes and Epigenomes for Enhanced Cancer Diagnosis by Cell-Free DNA. Nature Communications. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10085982/. Oh, Dongpin, Seth Strattan, Junho Hur, and et al. 2020. CNN-Peaks: ChIP-Seq Peak Detection Pipeline Using Convolutional Neural Networks That Imitate Human Visual Inspection. Springer Nature. https://www.nature.com/articles/s41598-020-64655-4. Patterson, Josh, and Adam Gibson. 2017. Deep Learning: A Practitioner’s Approach. O’Reilly Media, Inc. https://www.oreilly.com/library/view/deep-learning/9781491924570/. Toneyan, Shushan, Ziqi Tang, and Peter Koo. 2022. Evaluating Deep Learning for Predicting Epigenomic Profiles. Springer Nature. https://www.nature.com/articles/s42256-022-00570-9. Patterson and Gibson (2017)↩︎ Arisdakessian et al. (2019)↩︎ Avsec et al. (2021)↩︎ Oh et al. (2020)↩︎ Bae et al. (2017)↩︎ Toneyan, Tang, and Koo (2022)↩︎ "],["training-tricks.html", "4 Training tricks 4.1 Training on reverse complement 4.2 Training on sequence shifts", " 4 Training tricks 4.1 Training on reverse complement As explained in Part 1, DNA has a double helix structure. When we one-hot encode a segment of DNA in our models using a reference genome, we typically represent only one strand of the double helix. The complement of this strand is the opposite strand, where Adenine (A) pairs with Thymine (T), and Cytosine (C) pairs with Guanine (G). The reverse complement of a DNA strand is obtained by first taking its complement and then reading it in the reverse direction. This figure shows the reverse complement of a DNA sequence7 Training on DNA sequences and augmenting the data with their reverse complements has been shown to improve model accuracy, prediction, and interpretability in DNA sequence-related models. This approach involves “treating the reverse complement DNA sequence as another sample” (Cao and Zhang 2019). By incorporating reverse complements, the model is exposed to a wider variety of sequence patterns, which helps reduce overfitting and enhances generalization. As a result, models become better at recognizing patterns regardless of strand orientation. Although the logic to obtain the reverse complement of a DNA strand is straightforward, the Bio.Seq module from the Biopython library provides a simple way to do this. Augmenting your dataset with reverse complements is usually done to training sets, but can be applied to validation and test sets as well. from Bio.Seq import Seq # Example DNA sequence dna_sequence = Seq(&quot;ATGCGTAC&quot;) # Generate the reverse complement reverse_complement = dna_sequence.reverse_complement() print(&quot;Original Sequence: &quot;, dna_sequence) ## Original Sequence: ATGCGTAC print(&quot;Reverse Complement: &quot;, reverse_complement) ## Reverse Complement: GTACGCAT 4.2 Training on sequence shifts Training on small, random sequence shifts up and downstream by shifting the genomic coordinates of the input sequence is also known as jitter. Jittering adds diversity to the training data by creating slightly different versions of the same sequence. This allows models to be less sensitive to the exact positioning of features, making them more robust to variations in the data. It also allows models to generalise better to unseen data where sites may not always be perfectly aligned. A variation of jittering, called flanking “extends DNA sequences from its midpoint by X base pairs and takes the left, middle and right input windows of the extended sequence as training samples with the same labels, tripling the size of training set.8 note: A paper on evaluating deep learning for predicting epigenomic profiles found that models trained with augmentations (reverse complement and jittering), “yielded improved robustness, especially when trained on peak-centered data. On the other hand, models that were trained on coverage-threshold data already benefited from the randomly-centered profiles.”9 ’’’ How many epochs are typically used to train on? Which learning rates are commonly used? Hyper-parameter optimisation Raytune Successive halving algorithm (SHA) and Asynchronous SHA (ASHA) - ASHA is one of the current ‘best in class’ hyperparam opt How to optimise parameters when the models become huge, e.g. Enformer Celltyping How to do hyperparameter tuning when models become very large ’’’ References Cao, Zhen, and Shihua Zhang. 2019. Simple Tricks of Convolutional Neural Network Architectures Improve DNA–Protein Binding Prediction. Bioinformatics. https://academic.oup.com/bioinformatics/article/35/11/1837/5142724?login=false. Toneyan, Shushan, Ziqi Tang, and Peter Koo. 2022. Evaluating Deep Learning for Predicting Epigenomic Profiles. Springer Nature. https://www.nature.com/articles/s42256-022-00570-9. Youens-Clark, Ken. 2021. Mastering Python for Bioinformatics. O’Reily Media. https://www.oreilly.com/library/view/mastering-python-for/9781098100872/ch03.html. Youens-Clark (2021)↩︎ Cao and Zhang (2019)↩︎ Toneyan, Tang, and Koo (2022)↩︎ "],["choosing-which-genomic-regions-to-train-on.html", "5 Choosing which genomic regions to train on", " 5 Choosing which genomic regions to train on "],["effect-of-differences-in-sequencing-depths.html", "6 Effect of differences in sequencing depths", " 6 Effect of differences in sequencing depths "],["reproducibility-of-machine-learning-models.html", "7 Reproducibility of machine learning models 7.1 Seeding 7.2 Dashboarding", " 7 Reproducibility of machine learning models 7.1 Seeding 7.2 Dashboarding "],["testing.html", "8 Testing", " 8 Testing "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
