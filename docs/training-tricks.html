<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Training tricks | Machine Learning in Genomics: Containerised tutorials demonstrating best practises, pitfalls, and reproducibility</title>
  <meta name="description" content="a set of reproducible, containerized tutorials that include all necessary data, code, and descriptions to replicate key results, along with demonstrations of common pitfalls in the field of genomics." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Training tricks | Machine Learning in Genomics: Containerised tutorials demonstrating best practises, pitfalls, and reproducibility" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="a set of reproducible, containerized tutorials that include all necessary data, code, and descriptions to replicate key results, along with demonstrations of common pitfalls in the field of genomics." />
  <meta name="github-repo" content="https://github.com/sn2023imperial/genomicsmarkdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Training tricks | Machine Learning in Genomics: Containerised tutorials demonstrating best practises, pitfalls, and reproducibility" />
  
  <meta name="twitter:description" content="a set of reproducible, containerized tutorials that include all necessary data, code, and descriptions to replicate key results, along with demonstrations of common pitfalls in the field of genomics." />
  

<meta name="author" content=" Sach Nehal" />


<meta name="date" content="2024-09-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="loss-functions.html"/>
<link rel="next" href="reproducibility-of-machine-learning-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning in Genomics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="epigenetic-data.html"><a href="epigenetic-data.html"><i class="fa fa-check"></i><b>1</b> Epigenetic Data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="epigenetic-data.html"><a href="epigenetic-data.html#what-is-epigenetic-data"><i class="fa fa-check"></i><b>1.1</b> What is epigenetic data?</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="epigenetic-data.html"><a href="epigenetic-data.html#what-does-dna-look-like"><i class="fa fa-check"></i><b>1.1.1</b> What Does DNA Look Like?</a></li>
<li class="chapter" data-level="1.1.2" data-path="epigenetic-data.html"><a href="epigenetic-data.html#common-epigenetic-sequencing-techniques"><i class="fa fa-check"></i><b>1.1.2</b> Common Epigenetic Sequencing Techniques:</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="epigenetic-data.html"><a href="epigenetic-data.html#what-does-epigenetic-data-look-like"><i class="fa fa-check"></i><b>1.2</b> What does epigenetic data look like?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="epigenetic-data.html"><a href="epigenetic-data.html#representing-epigenetic-data"><i class="fa fa-check"></i><b>1.2.1</b> Representing epigenetic data</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="epigenetic-data.html"><a href="epigenetic-data.html#sources-of-epigenetic-data"><i class="fa fa-check"></i><b>1.3</b> Sources of epigenetic data</a></li>
<li class="chapter" data-level="1.4" data-path="epigenetic-data.html"><a href="epigenetic-data.html#ucscs-genome-browser"><i class="fa fa-check"></i><b>1.4</b> UCSC’S Genome Browser</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pre-processing-of-bigwig-files.html"><a href="pre-processing-of-bigwig-files.html"><i class="fa fa-check"></i><b>2</b> Pre-processing of bigWig files</a>
<ul>
<li class="chapter" data-level="2.0.1" data-path="pre-processing-of-bigwig-files.html"><a href="pre-processing-of-bigwig-files.html#transformations-to-stop-extreme-p-values"><i class="fa fa-check"></i><b>2.0.1</b> Transformations to stop extreme p-values</a></li>
<li class="chapter" data-level="2.1" data-path="pre-processing-of-bigwig-files.html"><a href="pre-processing-of-bigwig-files.html#data-loaders-and-simplifying-pre-processing"><i class="fa fa-check"></i><b>2.1</b> Data loaders and simplifying pre-processing</a></li>
</ul></li>
<li class="part"><span><b>II Training models with DNA input</b></span></li>
<li class="chapter" data-level="3" data-path="loss-functions.html"><a href="loss-functions.html"><i class="fa fa-check"></i><b>3</b> Loss functions</a></li>
<li class="chapter" data-level="4" data-path="training-tricks.html"><a href="training-tricks.html"><i class="fa fa-check"></i><b>4</b> Training tricks</a>
<ul>
<li class="chapter" data-level="4.1" data-path="training-tricks.html"><a href="training-tricks.html#reverse-complements-and-sequence-shifts"><i class="fa fa-check"></i><b>4.1</b> Reverse Complements and Sequence Shifts</a></li>
<li class="chapter" data-level="4.2" data-path="training-tricks.html"><a href="training-tricks.html#hyper-parameter-optimisation"><i class="fa fa-check"></i><b>4.2</b> Hyper-parameter optimisation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="reproducibility-of-machine-learning-models.html"><a href="reproducibility-of-machine-learning-models.html"><i class="fa fa-check"></i><b>5</b> Reproducibility of machine learning models</a></li>
<li class="chapter" data-level="6" data-path="testing.html"><a href="testing.html"><i class="fa fa-check"></i><b>6</b> Testing</a></li>
<li class="chapter" data-level="7" data-path="software-libraries-for-model-building.html"><a href="software-libraries-for-model-building.html"><i class="fa fa-check"></i><b>7</b> Software libraries for model building</a>
<ul>
<li class="chapter" data-level="7.1" data-path="software-libraries-for-model-building.html"><a href="software-libraries-for-model-building.html#grelu"><i class="fa fa-check"></i><b>7.1</b> gReLU</a></li>
<li class="chapter" data-level="7.2" data-path="software-libraries-for-model-building.html"><a href="software-libraries-for-model-building.html#kipoi"><i class="fa fa-check"></i><b>7.2</b> Kipoi</a></li>
</ul></li>
<li class="part"><span><b>III ML pitfalls in genomics</b></span></li>
<li class="chapter" data-level="8" data-path="pitfalls-overview.html"><a href="pitfalls-overview.html"><i class="fa fa-check"></i><b>8</b> Pitfalls overview</a></li>
<li class="part"><span><b>IV Using existing models</b></span></li>
<li class="chapter" data-level="9" data-path="training-grelu-models-and-examining-pitfalls.html"><a href="training-grelu-models-and-examining-pitfalls.html"><i class="fa fa-check"></i><b>9</b> Training gReLU models and examining pitfalls</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning in Genomics: Containerised tutorials demonstrating best practises, pitfalls, and reproducibility</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="training-tricks" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Training tricks<a href="training-tricks.html#training-tricks" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="reverse-complements-and-sequence-shifts" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Reverse Complements and Sequence Shifts<a href="training-tricks.html#reverse-complements-and-sequence-shifts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Reverse Complements</strong> <br></p>
<p>As explained in Part 1, DNA has a double helix structure. When we one-hot encode a segment of DNA in our models using a reference genome, we typically represent only one strand of the double helix. The complement of this strand is the opposite strand, where Adenine (A) pairs with Thymine (T), and Cytosine (C) pairs with Guanine (G). The reverse complement of a DNA strand is obtained by first taking its complement and then reading it in the reverse direction.</p>
<p><br></p>
<div style="text-align: center;">
<p><img src="images/reverse_complement.png" alt="Reverse Complement" width="300" /></p>
<p>This figure shows the reverse complement of a DNA sequence<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a></p>
</div>
<p><br></p>
<p>Training on DNA sequences and augmenting the data with their reverse complements has been shown to improve model accuracy, prediction, and interpretability in DNA sequence-related models. This approach involves “treating the reverse complement DNA sequence as another sample”<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a>. By incorporating reverse complements, the model is exposed to a wider variety of sequence patterns, which helps reduce overfitting and enhances generalization. As a result, models become better at recognizing patterns regardless of strand orientation. Although the logic to obtain the reverse complement of a DNA strand is straightforward, the Bio.Seq module from the Biopython library provides a simple way to do this. Augmenting your dataset with reverse complements is usually done to training sets, but can be applied to validation and test sets as well.
<br></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="training-tricks.html#cb2-1" tabindex="-1"></a><span class="im">from</span> Bio.Seq <span class="im">import</span> Seq</span>
<span id="cb2-2"><a href="training-tricks.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="training-tricks.html#cb2-3" tabindex="-1"></a><span class="co"># Example DNA sequence</span></span>
<span id="cb2-4"><a href="training-tricks.html#cb2-4" tabindex="-1"></a>dna_sequence <span class="op">=</span> Seq(<span class="st">&quot;ATGCGTAC&quot;</span>)</span>
<span id="cb2-5"><a href="training-tricks.html#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="training-tricks.html#cb2-6" tabindex="-1"></a><span class="co"># Generate the reverse complement</span></span>
<span id="cb2-7"><a href="training-tricks.html#cb2-7" tabindex="-1"></a>reverse_complement <span class="op">=</span> dna_sequence.reverse_complement()</span>
<span id="cb2-8"><a href="training-tricks.html#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="training-tricks.html#cb2-9" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Original Sequence: &quot;</span>, dna_sequence)</span></code></pre></div>
<pre><code>## Original Sequence:  ATGCGTAC</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="training-tricks.html#cb4-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Reverse Complement: &quot;</span>, reverse_complement)</span></code></pre></div>
<pre><code>## Reverse Complement:  GTACGCAT</code></pre>
<p><br></p>
<p><strong>Sequence Shifts</strong> <br></p>
<p>Training on small, random sequence shifts up and downstream by shifting the genomic coordinates of the input sequence is also known as jitter. Jittering adds diversity to the training data by creating slightly different versions of the same sequence. This allows models to be less sensitive to the exact positioning of features, making them more robust to variations in the data. It also allows models to generalise better to unseen data where sites may not always be perfectly aligned. A variation of jittering, called flanking “extends DNA sequences from its midpoint by X base pairs and takes the left, middle and right input windows of the extended sequence as training samples with the same labels, tripling the size of training set.<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a>
<br></p>
<p>Implementing data augmentations using reverse complements and sequence shifts can be approached in different ways. Similar to the ‘flanking’ example, you can either expand your dataset by adding additional augmented data points or apply a random augmentation strategy, where only some data points are randomly augmented while keeping the total number of points in your dataset unchanged. Data augmentations are usually applied only to training sets, however in the context of computer vision “many research reports have shown the effectiveness of augmenting data at test-time as well”.<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a> When implementing augmentations like reverse complements and sequence shifts, these are typically applied after splitting your data into training, validation, and test sets. When applying sequence shifting, it’s logical to shift the interval before retrieving the nucleotide sequence from the reference genome. The reverse complement should be applied after retrieving the nucleotide sequence but before one-hot encoding it. If you’re using the BioPython library, this works well since BioPython’s reverse complement function operates on string inputs.
<br></p>
<p>In the genomics context, a paper on evaluating deep learning for predicting epigenomic profiles used two convolutional neural networks, <a href="https://github.com/calico/basenji" target="_blank">Basenji</a> and <a href="https://github.com/kundajelab/bpnet" target="_blank">BPNet</a>, trained on ATAC-seq data, to predict coverage values as a regression. They found that convolutional models trained with augmentations (reverse complement and sequence shifts), “yielded improved robustness, especially when trained on peak-centered data (BPNet). On the other hand, models that were trained on coverage-threshold data (Basenji) already benefited from the randomly-centered profiles.”<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a> Additionally, while they initially “used a MSE and multinomial NLL loss for BPNet, [they] found that optimization using Poisson NLL yielded better performance.”<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a> This finding is another motivation of using at poisson loss function in subsequent tutorials.
<br></p>
</div>
<div id="hyper-parameter-optimisation" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Hyper-parameter optimisation<a href="training-tricks.html#hyper-parameter-optimisation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Which learning rates are commonly used? How many epochs are typically used to train on?<br></p>
<p>While the learning rate and number of epochs differ by model and study, based on some of the research cited so far, common learning rates are in the range 1e-4<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a> to 1e-3<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a>. Additionally, some studies apply learning rate decay if the loss function shows no improvement over time<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a> while others lower the learning rate for fine tuning.<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a>
<br></p>
<p>The number of epochs used to train on differs by quite a margin. In training a convolution neural network to explore the effects of genomic data augmentation, 30 epochs were used.<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a> DeepImpute which constructs multiple sub-neural networks for genotype imputation, trains on a maximum of 500 epochs, while the study involving the Basenji and BPNet models were trained on a maximum of 100 epochs. The clear strategy for these larger models involve the use of early stopping if no improvements are evident after 5-10 epochs.
<br></p>
<p>When hyperparameter optimising, the consensus for achieving the best model performance is to train with a high number of epochs to enable the model to confidently learn features as they apply to labels, starting with a high learning rate, and decreasing over time using a learning rate scheduler. Interestingly, a study on binary peak detection using CNNs on ChIP-Seq data manually tuned their model’s hyperparameters and found that little changes in performance results<a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a>. This highlights the challenges of hyperparameter tuning with larger models, where manually fine-tuning is not ideal. How can hyperparameter tuning on these larger models be done in practice?
<br><br></p>
<p><strong>Raytune</strong></p>
<p><a href="https://docs.ray.io/en/latest/tune/index.html" target="_blank">Raytune</a> “is a Python library for experiment execution and hyperparameter tuning at any scale”. It aids in leveraging state of the art hyperparameter optimisation algorithms while simplifying scaling for larger models. Raytune hyperparameter searching can also be scaled to cloud based clusters without the need for large changes in code structure. Additionally, it supports several machine learning frameworks such as pyTorch and TensorFlow<a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a>. One of the strongest current hyperparameter optimisation algorithms is the Asynchronous Successive Halving Algorithm or <strong><a href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" target="_blank">ASHA</a></strong>. Asha “exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems”<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a>, allowing for faster optimisation and applicability to the larger models common in genomics. A study on predicting the impact of sequence motifs on gene regulation utilised Raytune and the ASHA algorithm to successfully optimise their model’s hyperparameters<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a>.
<br></p>
<p>In the genomic context, as a result of complex models using large genomic datasets, hyperparameter tuning using a brute force approach is untenable. Utilising existing libraries such as Raytune and incorporating asynchronous algorithms such as ASHA, has the potential to pave the way forward in improving model performance without unreasonable computational costs.</p>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-cedric2019" class="csl-entry">
Arisdakessian, Cédric, Olivier Poirion, Breck Yunits, Xun Zhu, and Lana Garmire. 2019. <em>DeepImpute: An Accurate, Fast, and Scalable Deep Neural Network Method to Impute Single-Cell RNA-Seq Data</em>. Springer Nature. <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1837-6">https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1837-6</a>.
</div>
<div id="ref-avsec2021" class="csl-entry">
Avsec, Žiga, Vikram Agarwal, Daniel Visentin, and et al. 2021. <em>Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions</em>. Springer Nature. <a href="https://www.nature.com/articles/s41592-021-01252-x">https://www.nature.com/articles/s41592-021-01252-x</a>.
</div>
<div id="ref-cao2019" class="csl-entry">
Cao, Zhen, and Shihua Zhang. 2019. <em>Simple Tricks of Convolutional Neural Network Architectures Improve DNA–Protein Binding Prediction</em>. Bioinformatics. <a href="https://academic.oup.com/bioinformatics/article/35/11/1837/5142724?login=false">https://academic.oup.com/bioinformatics/article/35/11/1837/5142724?login=false</a>.
</div>
<div id="ref-jacob2023" class="csl-entry">
Hepkema, Jacob, Nicholas Lee, Benjamin Stewart, and et al. 2023. <em>Predicting the Impact of Sequence Motifs on Gene Regulation Using Single-Cell Data</em>. National Library of Medicine. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10426127/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10426127/</a>.
</div>
<div id="ref-liam2018" class="csl-entry">
Li, Liam. 2018. <em>A System for Massively Parallel Hyperparameter Tuning</em>. Conference on Machine Learning; Systems. <a href="https://arxiv.org/abs/1810.05934">https://arxiv.org/abs/1810.05934</a>.
</div>
<div id="ref-liaw2018" class="csl-entry">
Liaw, Richard, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez, and Ion Stoica. 2018. <span>“Tune: A Research Platform for Distributed Model Selection and Training.”</span> <em>arXiv Preprint arXiv:1807.05118</em>.
</div>
<div id="ref-oh2020" class="csl-entry">
Oh, Dongpin, Seth Strattan, Junho Hur, and et al. 2020. <em>CNN-Peaks: ChIP-Seq Peak Detection Pipeline Using Convolutional Neural Networks That Imitate Human Visual Inspection</em>. Springer Nature. <a href="https://www.nature.com/articles/s41598-020-64655-4">https://www.nature.com/articles/s41598-020-64655-4</a>.
</div>
<div id="ref-connor2019" class="csl-entry">
Shorten, Connor, and Taghi Khoshgoftaar. 2019. <em>Design Considerations for Image Data Augmentation</em>. Journal of Big Data. <a href="https://link.springer.com/article/10.1186/s40537-019-0197-0?">https://link.springer.com/article/10.1186/s40537-019-0197-0?</a>
</div>
<div id="ref-toneyan2022" class="csl-entry">
Toneyan, Shushan, Ziqi Tang, and Peter Koo. 2022. <em>Evaluating Deep Learning for Predicting Epigenomic Profiles</em>. Springer Nature. <a href="https://www.nature.com/articles/s42256-022-00570-9">https://www.nature.com/articles/s42256-022-00570-9</a>.
</div>
<div id="ref-clark2021" class="csl-entry">
Youens-Clark, Ken. 2021. <em>Mastering Python for Bioinformatics</em>. O’Reily Media. <a href="https://www.oreilly.com/library/view/mastering-python-for/9781098100872/ch03.html">https://www.oreilly.com/library/view/mastering-python-for/9781098100872/ch03.html</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="29">
<li id="fn29"><p><span class="citation">Youens-Clark (<a href="#ref-clark2021">2021</a>)</span><a href="training-tricks.html#fnref29" class="footnote-back">↩︎</a></p></li>
<li id="fn30"><p><span class="citation">Cao and Zhang (<a href="#ref-cao2019">2019</a>)</span><a href="training-tricks.html#fnref30" class="footnote-back">↩︎</a></p></li>
<li id="fn31"><p><span class="citation">Cao and Zhang (<a href="#ref-cao2019">2019</a>)</span><a href="training-tricks.html#fnref31" class="footnote-back">↩︎</a></p></li>
<li id="fn32"><p><span class="citation">Shorten and Khoshgoftaar (<a href="#ref-connor2019">2019</a>)</span><a href="training-tricks.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p><span class="citation">Toneyan, Tang, and Koo (<a href="#ref-toneyan2022">2022</a>)</span><a href="training-tricks.html#fnref33" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p><span class="citation">Toneyan, Tang, and Koo (<a href="#ref-toneyan2022">2022</a>)</span><a href="training-tricks.html#fnref34" class="footnote-back">↩︎</a></p></li>
<li id="fn35"><p><span class="citation">(<a href="#ref-cedric2019">Arisdakessian et al. 2019</a>)</span>, <span class="citation">(<a href="#ref-avsec2021">Avsec et al. 2021</a>)</span><a href="training-tricks.html#fnref35" class="footnote-back">↩︎</a></p></li>
<li id="fn36"><p><span class="citation">(<a href="#ref-cao2019">Cao and Zhang 2019</a>)</span>, <span class="citation">(<a href="#ref-toneyan2022">Toneyan, Tang, and Koo 2022</a>)</span><a href="training-tricks.html#fnref36" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p><span class="citation">Toneyan, Tang, and Koo (<a href="#ref-toneyan2022">2022</a>)</span><a href="training-tricks.html#fnref37" class="footnote-back">↩︎</a></p></li>
<li id="fn38"><p><span class="citation">Avsec et al. (<a href="#ref-avsec2021">2021</a>)</span><a href="training-tricks.html#fnref38" class="footnote-back">↩︎</a></p></li>
<li id="fn39"><p><span class="citation">Cao and Zhang (<a href="#ref-cao2019">2019</a>)</span><a href="training-tricks.html#fnref39" class="footnote-back">↩︎</a></p></li>
<li id="fn40"><p><span class="citation">Oh et al. (<a href="#ref-oh2020">2020</a>)</span><a href="training-tricks.html#fnref40" class="footnote-back">↩︎</a></p></li>
<li id="fn41"><p><span class="citation">Liaw et al. (<a href="#ref-liaw2018">2018</a>)</span><a href="training-tricks.html#fnref41" class="footnote-back">↩︎</a></p></li>
<li id="fn42"><p><span class="citation">Li (<a href="#ref-liam2018">2018</a>)</span><a href="training-tricks.html#fnref42" class="footnote-back">↩︎</a></p></li>
<li id="fn43"><p><span class="citation">Hepkema et al. (<a href="#ref-jacob2023">2023</a>)</span><a href="training-tricks.html#fnref43" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="loss-functions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="reproducibility-of-machine-learning-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-training-models-dna.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
