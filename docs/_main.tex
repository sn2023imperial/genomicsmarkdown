% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Machine Learning in Genomics: Containerised tutorials demonstrating best practises, pitfalls, and reproducibility},
  pdfauthor={ Sach Nehal},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Machine Learning in Genomics: Containerised tutorials demonstrating best practises, pitfalls, and reproducibility}
\author{Sach Nehal}
\date{2024-09-03}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{About}\label{about}
\addcontentsline{toc}{chapter}{About}

Applied machine learning utilising vast amounts of data has aided in pattern identification, predictive analytics, and solving complex problems across a multitude of fields. Solving these complex problems within these fields, researchers would find differing answers to the following questions; \textbf{what machine learning techniques can we apply to the problem, how do we apply the techniques in the context of this field, and why do we need to apply them in this way?} In any case, applied machine learning requires an interdisciplinary understanding of computing techniques and the field in question.

The aim of this project is to provide you with \textbf{a set of reproducible tutorials that include all necessary data, code, and descriptions to replicate key results, along with demonstrations of common pitfalls, in the field of genomics}. It is designed for users with knowledge of machine learning but little or no background in biology as a process to learn about applying machine learning techniques in genomics.

\part{Introduction}\label{part-introduction}

\chapter{Epigenetic Data}\label{epigenetic-data}

\section{What is epigenetic data?}\label{what-is-epigenetic-data}

As you may already know, typically all of the cells in your body contain the same DNA. How, then, do we have different cell types in our body? Your DNA contains a script that is able to produce the proteins required for each specific cell in your body. Which proteins, and subsequently which cells are made, depends on gene expression and regulation, i.e.~``the way each cell deploys its genome.''\footnote{\citet{ralston2008}}

\textbf{\emph{Epigenetic data}} arises from ``the study of heritable and stable changes in gene expression that occur through alterations in the chromosome rather than in the DNA sequence.''\footnote{\citet{nora2023}}

\includegraphics{images/Epigenetic_Mechanisms.png}

\href{https://commonfund.nih.gov/sites/default/files/epigeneticmechanisms.pdf}{commonfund.nih.gov}

The image above shows quite simply the basics of genetic structures. Several more complex processes are involved during cell replication such as DNA transcription and translation in order to make proteins. A key takeaway in coming closer to understanding gene expression is that \textbf{Chromatin} is a complex structure made up of DNA wound around histone proteins, with some segments of DNA being accessible/inaccessible to further processes. \textbf{Euchromatin} refers to the accessible state, while \textbf{Heterochromatin} refers to a chromatin state in which DNA cannot be transcribed (inaccessible).\footnote{\citet{shahid2023}} There are many different epigenetic modifications that affect chromatin accessibility.

Some common epigenetic modifications include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{DNA Methylation}: Addition of methyl groups to DNA, affecting gene expression regulation\footnote{\citet{nora2023}}.
\item
  \textbf{Histone Modifications}: Chemical changes to histone proteins that DNA wraps around, including acetylation, methylation, or phosphorylation. These changes influence chromatin structure and gene accessibility.\footnote{\citet{Kouzarides2007}}
\item
  \textbf{Chromatin Accessibility}: Regions of open chromatin that are accessible to transcription factors (special types of proteins that bind to DNA sequences and regulate gene expression) further dictate which regions of DNA can be expressed\footnote{\citet{melanie2021}}.
\end{enumerate}

In studying gene expression and epigenetic modifications, we aim to more closely understand biological mechanisms that regulate development, disease, and how cells respond to epigenetic factors.

\subsection{What Does DNA Look Like?}\label{what-does-dna-look-like}

As illustrated in the image above, DNA is structured as a double helix, with two complementary strands intertwined to form the characteristic helical shape. DNA consists of an extremely long sequence composed of four types of nucleotides: Adenine (A), Cytosine (C), Thymine (T), and Guanine (G).

According to the National Cancer Institute (USA), nucleotides within the DNA double helix form complementary pairs---Adenine pairs with Thymine, and Guanine pairs with Cytosine\footnote{\citet{ncidefinitions}}. These pairs are commonly referred to as base pairs (bps). For example, if one strand of the double helix has the sequence ``ATCGG'', the complementary strand will have the sequence ``TAGCC''.

Genes are sequences of DNA located at specific positions on chromosomes and can vary in length. Each gene encodes information necessary for producing proteins or RNA molecules, which are essential for the structure, function, and regulation of an organism\footnote{\citet{ncidefinitions}}. The complete set of genetic material in an organism is known as its genome.

\includegraphics{images/dna_sequence.png}

Image highlighting part of a \href{https://www.researchgate.net/profile/Pratik-Kanani/publication/341901570/figure/fig1/AS:898621708984321@1591259519483/A-human-DNA-and-Part-of-DNA-sequence-28-29.jpg}{dna sequence and base pairs}.\footnote{\citet{kanani2020}}

\subsection{Common Epigenetic Sequencing Techniques:}\label{common-epigenetic-sequencing-techniques}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{\emph{ATAC-Seq}} (Assay for Transposase-Accessible Chromatin with Sequencing):
  o\texttt{Measures\ chromatin\ accessibility\ to\ identify\ open\ regions\ of\ the\ genome\ where\ transcription\ factors\ can\ bind.}
  o\texttt{Output:\ Peaks\ indicating\ accessible\ chromatin\ regions.}
\item
  \textbf{\emph{ChIP-Seq}} (Chromatin Immunoprecipitation Sequencing):
  o\texttt{Used\ to\ identify\ DNA\ regions\ bound\ by\ specific\ proteins\ (e.g.,\ transcription\ factors,\ histones\ with\ specific\ modifications).}
  o\texttt{Output:\ Peaks\ indicating\ binding\ sites\ or\ modification\ locations.}
\end{enumerate}

\section{What does epigenetic data look like?}\label{what-does-epigenetic-data-look-like}

Epigenetic data can be represented in various forms, depending on the type of modification being studied and the methods used to gather the data. \textbf{ATAC-Seq} and \textbf{ChIP-Seq} are the common methods I will focus on, but there are others that may produce different forms of data, such as WGS (whole-genome sequencing) which produces nucleotide sequencing data, or Bisulfite conversion of DNA producing data on methylation levels across the genome.

\subsection{Representing epigenetic data}\label{representing-epigenetic-data}

Epigenetic data originates from sequencing methods such as ATAC-Seq or ChIP-Seq experiments. The initial experiments produce raw sequencing reads (fragments), which are then aligned to a reference genome. By aligning these sequences, we can aggregate the reads into regions where they `pile up' to form peaks, indicating areas of significant biological activity or modification. This can be done per base across the genome, or per gene. Additionally, we could also examine mismatches where a read's base differs from the genome's base, and use them to identify SNPs (\href{https://www.cancer.gov/publications/dictionaries/genetics-dictionary/def/single-nucleotide-polymorphism}{single nucleotide polymorphisms})\footnote{\citet{ncidefinitions}}. This mismatch information can be recorded in a table showing the position, type of mismatch, and the number of reads supporting each mismatch.\footnote{\citet{akalin2020}}

\includegraphics{images/sequencing_pipeline.png}

Image showing the \href{https://compgenomr.github.io/book/images/HTseq.png}{sequencing pipeline} from high-throughput sequencing methods\footnote{\citet{akalin2020}}.

1. \textbf{\emph{Raw Sequence Reads:}}
o\texttt{These\ are\ the\ basic\ output\ of\ sequencing\ experiments,\ such\ as\ those\ from\ ChIP-Seq\ or\ ATAC-Seq.}
o\texttt{Reads\ are\ processed\ and\ aligned\ to\ a\ reference\ genome\ before\ undergoing\ peak\ calling.}

Lets look at what a few lines of raw sequence read data consists of:
The data is taken from Encode Experiment \href{https://www.encodeproject.org/experiments/ENCSR817LUF/}{ENCSR817LUF} (chIP-Seq). The accession ID of the raw sequence read data is \href{https://www.encodeproject.org/files/ENCFF397NRK/}{ENCFF397NRK}. Genomic data comes in many file formats. This specific raw sequence read data is a compressed FASTQ file.

Note: The script I used involved streaming the data directly from a URL using the requests library. While files containing genomic data are generally quite large, for computational efficiency it is recommended that data be downloaded locally.

\begin{verbatim}
## ID: B091JABXX110402:1:2204:12975:184709
## Sequence: GTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGG
## Quality Scores: [31, 30, 30, 32, 36, 37, 36, 32, 33, 32, 35, 36, 37, 33, 33, 34, 37, 37, 36, 33, 34, 30, 37, 37, 37, 33, 34, 33, 38, 33, 33, 32, 33, 35, 37, 36]
## 
## ID: B091JABXX110402:1:2205:18641:8399
## Sequence: GGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAG
## Quality Scores: [22, 27, 31, 30, 30, 33, 31, 32, 31, 31, 24, 36, 37, 36, 33, 34, 33, 37, 37, 32, 32, 34, 22, 37, 36, 30, 23, 35, 27, 29, 28, 28, 23, 31, 26, 35]
## 
## ID: B091JABXX110402:1:1207:12202:100922
## Sequence: AGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTT
## Quality Scores: [33, 33, 36, 37, 31, 34, 34, 36, 37, 36, 29, 33, 35, 36, 39, 37, 32, 33, 35, 37, 34, 37, 33, 34, 36, 38, 39, 25, 32, 35, 36, 38, 37, 38, 32, 32]
\end{verbatim}

As you can see, each data entry is a DNA sequence (read). While I'm only showing the first three entries, for each experiment there are millions of sequencing reads. The quality scores indicate the confidence of each base call in the sequence. Higher scores suggest higher confidence. The scores are in Phred format, where a score of 20 corresponds to a 99\% base call accuracy, 30 corresponds to 99.9\%, 40 corresponds to 99.99\%, and so on.\footnote{\citet{green}}
2. \textbf{\emph{Peak Calling:}}
o\texttt{A\ method\ used\ to\ identify\ regions\ in\ the\ genome\ where\ there\ is\ significant\ enrichment\ of\ sequencing\ reads.\ This\ indicates\ the\ presence\ of\ DNA-protein\ interactions\ (e.g.,\ transcription\ factor\ binding\ sites\ or\ accessible\ chromatin\ regions).}
o\texttt{Peaks\ represent\ areas\ where\ epigenetic\ marks\ or\ chromatin\ accessibility\ are\ concentrated.}

A common peak calling algorithm is \href{https://hbctraining.github.io/Intro-to-ChIPseq/lessons/05_peak_calling_macs.html}{MACS2}. Essentially, aligned sequencing reads are aggregated into regions where they `pile up' to form peaks as a read count per base. The outputs typically include signal p-values or fold change over control values, representing the expectation of a peak. Signal p-values are negative log transformed resulting in -log10 signal p-values. While MACS2 is traditionally used in ChIP-seq experiments, it is also applied to ATAC-seq to identify significant peaks and assess their enrichment.\footnote{\citet{mistry2022}} In ChIP-seq, broad peaks often represent histone modifications covering entire gene bodies, while narrow peaks are indicative of transcription factor binding sites. In ATAC-seq, the peaks primarily reflect regions of open chromatin.\footnote{\citet{wilbanks2010}} Peak calling reduces background noise and utilises signal smoothing techniques to more accurately detect peaks. When using -log10 p-value and fold change data, base pair averaging is commonly used.

Imagine you have a set of read coverage data from a sequencing experiment. At each base pair position along a chromosome, you have a read count representing how many times that base pair was sequenced. However, due to technical noise, these counts might fluctuate wildly from one base to the next. By applying base pair averaging over a window (e.g.~32bp), you might see that while individual counts vary, there is a broader region where the average read coverage is consistently high, indicating a potential region of interest. The intended effect of base pair averaging is further reducing noise, and signal smoothing.
\includegraphics{images/bp_resolution.png}

Figure showing a 32 base pair resolution following base pair averaging of CHip-seq data\footnote{\citet{patel2024}}

As you will see in the tutorials, you can re-average your data to a higher base pair average before using it in machine learning models. This can be implemented to decrease dimensionality, reduce computational intensity and tailor the model to understanding regions of a certain scale.

\textbf{\emph{Representing Peaks:}}
o \textbf{\emph{P-value or Fold-change:}}
P-value: Indicates the statistical significance of the peak, helping to distinguish true peaks from background noise.
Fold-change: Represents the difference in read density between treated and control samples, indicating the strength of the signal.

\includegraphics{images/coverage_p.png}
This image shows the signal p-value coverage over a small region (11,176bps) in chromosome one from Encode Experiment \href{https://www.encodeproject.org/experiments/ENCSR817LUF/}{ENCSR817LUF} (The same chIP-Seq experiment we saw the raw sequence reads from). For further context, experiment ENCSR817LUF targets the H3K36me3 histone modification in brain tissue. The experiment aims to map the locations where the H3K36me3 histone modification is present along the genome. Therefore the peaks represent regions of the genome where the the H3K36me3 histone modification is enriched compared to the background or control. The accession ID of the signal p-value data is \href{https://www.encodeproject.org/files/ENCFF601VTB/}{ENCFF601VTB}. Genomic data comes in many file formats. This specific signal p-value data is a bigWig file.
o \textbf{\emph{Types of Peaks:}}
Categorical Peaks: Simple yes/no indication of a peak's presence.
Continuous Peaks: More nuanced representation that includes the intensity or enrichment level of the peak, often visualized as a signal track.
Thresholded/Pseudoreplicated Peaks: Usually categorical, these peaks are of high confidence regions from multiple replicates (experiments) or pseudoreplicates (artificial data splits), to ensure reliability and reproducibility.

\textbf{\emph{Example Data Pipeline}}

\href{https://www.encodeproject.org/experiments/ENCSR817LUF/}{encodeproject.org}

This is an example data pipeline from Encode Experiment \href{https://www.encodeproject.org/experiments/ENCSR817LUF/}{ENCSR817LUF}, the same chIP-Seq experiment we saw the raw sequence reads, and signal p-value coverage from. The yellow bubbles represent downloadable data sets of different types, while the blue boxes represent step types (e.g.~peak calling). In the left column are multiple data sets of raw sequence reads, which then undergo data quality steps before being aligned (first blue box) to the reference human genome GRCh38 (denoted by ENCFF110MCL below the reads). The next steps include Peak calling (categorical peaks) and signal generation (continuous peaks) to produce the data we normally use in our machine learning models. This data pipeline process aids in normalisation, noise reduction, and dimensionality reduction of the data.

\subsection{Transformations to stop extreme p-values}\label{transformations-to-stop-extreme-p-values}

When utlising genomic data which incorporates p-values, it is important to consider and deal with extreme p-values. One way this is done is through using an Arcsinh-transformation (inverse hyperbolic sine).
\(\text{arsinh}(x) = \ln \left( x + \sqrt{x^2 + 1} \right)\)

The arcsinh-transformation as a logarithmic function helps in reducing the significance of outliers and sequencing depth while maintaining variance by compressing the range of the data. This transformation can be used in the data preprocessing stage. The graph below visualises how the transformation works. While extreme values are transformed logarithmically, the smaller values are barely transformed as the function for smaller values is more linear in nature.

\includegraphics{images/arcsinh.png}

Plot of \href{https://miro.medium.com/v2/resize:fit:1100/format:webp/1*glJtHk1HRZpYHsk79QxgwQ.png}{Arcsinh Transformation} compared to a log function, made with Desmos available under \href{https://miro.medium.com/v2/resize:fit:1100/format:webp/1*glJtHk1HRZpYHsk79QxgwQ.png}{CC BY-SA 4.0.} Text, arrows, and box shape added to image.

\section{Sources of epigenetic data}\label{sources-of-epigenetic-data}

There are numerous public data banks which contain genomic datasets ready to be downloaded.
\href{https://projects.ensembl.org/blueprint/}{Blueprint}
Blueprint's genomic datasets are focused on gene expression in healthy and diseased cells mostly relating to haematopoietic cells (cells which develop into different types of blood cells).

\href{https://www.ncbi.nlm.nih.gov/geo/roadmap/epigenomics/}{Roadmap}
The National Institute of Health's Roadmap Epigenomics Project contains sample datasets from multiple experiements as well as reference and mapping datasets.

\href{https://www.encodeproject.org/}{Encode}
The Encode Project contains a large amount of publicly available genomic data easily filtered and downloaded. The genomic data used in this markdown book is sourced from Encode.

The largest genomic data bank is the \href{https://www.ukbiobank.ac.uk/}{UK Biobank}, however they require that you apply for access to their datasets.

\section{UCSC'S Genome Browser}\label{ucscs-genome-browser}

The UCSC Genome Browser is a powerful and versatile tool that allows the visualisation and exploration of many sets of genomic data, especially bigWig files. It offers an extensive collection of genome assemblies, annotation tracks, and functional data, enabling users to examine gene structures, regulatory elements, and genetic variations. With its user-friendly interface and customisable display options, the UCSC Genome Browser facilitates detailed genomic analyses and supports a wide range of applications in genomics and bioinformatics. Whether you're investigating gene functions, exploring genetic variants, or studying comparative genomics, the UCSC Genome Browser serves as an essential resource for understanding complex genomic information. It is also possible to load and visualise genomic data from other sources such as Encode. While the visualisations are extensive, as you can explore below, the browser can be quite overwhelming for first time users.

The following is an example of what the same chIP-Seq data targeting the H3K36me3 histone modification in brain tissue looks like using UCSC's Genome Browser. The pseudoreplicated peaks represent categorically, the significant locations along the genome where the H3K36me3 histone modification is present.

\href{https://genome.ucsc.edu/cgi-bin/hgTracks?db=hg38&lastVirtModeType=default&lastVirtModeExtraState=&virtModeType=default&virtMode=0&nonVirtPosition=&position=chr1\%3A11084744\%2D11095920&hgsid=2307713234_Kap236Tjt6ZGnnNrXMkIhq2Ajn27}{UCSC Genome Browser}

The following is an example of ATAC-Seq data from an experiment on T-helper 17 cells (a type of immune system cell). Recall that the ATAC-Seq method aims to find chromatin regions that are accessible for transcription factor binding. The p-value and fold change graphs show continuous peaks, while the IDR thresholded peaks and pseudoreplicated peaks represent the significant locations of accessible chromatin along the genome.

\href{https://genome.ucsc.edu/cgi-bin/hgTracks?db=hg38&lastVirtModeType=default&lastVirtModeExtraState=&virtModeType=default&virtMode=0&nonVirtPosition=&position=chr1\%3A88379533\%2D113275174&hgsid=2307721306_mcnECXS4Hy0fNQ4yz3ZQTL7nimkW}{UCSC Genome Browser}

\chapter{Pre-processing of bigWig files}\label{pre-processing-of-bigwig-files}

BigWig files containing signal p-value or fold change data can be quite tricky to deal with. However, libraries such as pyBigWig enable easier access of data. In order to understand how to handle the data pre-processing stage, I have created a jupyter notebook tutorial on Google Colab. The tutorial begins using UCSC's programs to quickly understand the genomic data within BigWigs, before using the pyBigWig library to simply extract BigWig data.

The final part of the tutorial uses the pyBigWig library to load, filter, and split BigWig data into training, validation, and test sets. The data consists of signal p-values from ChIP-seq experiments, processed using the MACS2 tool. We will re-average these signals to a resolution of 32 base pairs. Additionally, we will implement threshold-based filtering and consistent data splits to understand how to ready data for a model.

Tutorial 1: Loading and Pre-Processsing Data from bigWigs (interactive)

Tutorial 1: Loading and Pre-Processsing Data from bigWigs (nbviewer)

\section{Data loaders and simplifying pre-processing}\label{data-loaders-and-simplifying-pre-processing}

Data loaders are scripts/functions to load batches of data into your model. They are crucial in machine learning because they simplify how data is fed into models, making the whole process smoother and more efficient. This becomes especially important with the large datasets used in genomic studies, where managing and processing data manually would be cumbersome. By automating these tasks, data loaders help ensure that data is processed efficiently, allowing for faster and more effective model training. While there are existing github repositories with data loaders, such as \href{https://kipoi.org/kipoiseq/dataloaders/}{``Kipoi Dataloader''}, and \href{https://github.com/pfizer-opensource/bigwig-loader/blob/main/README.md}{``Dataloader for BigWig files''}.\footnote{\citet{retel_fast_2024}}, depending on the data used and model you build, they won't cover all of the use cases. When building one yourself, the PyTorch library has its own \href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{dataset and dataloader} modules, which include creating custom datasets.

\section{Dealing with missing data (oversampling, undersampling, weighting)}\label{dealing-with-missing-data-oversampling-undersampling-weighting}

In genomics, class imbalance is a frequent challenge, often necessitating the use of statistical methods to validate the few positive instances amid vast amounts of data. This is particularly evident in tasks such as alignment queries, GWAS projects, and motif scanning, where conservative significance thresholds are essential to control false positives due to the low frequency of true positives across the genome. Researchers tend to address these imbalances by either oversampling the minority class, undersampling the majority class or by employing weighting.\footnote{\citet{whalen2022}}

In Tutorial 1, we utilised thresholding to filter our data to focus on regions with significant coverage. While there were around 300,000 bins across the three chromosomes we looked at, after thresholding our data consisted of roughly 10\% or 30,000 bins. Our data does not contain any coverage values below the threshold. In the pitfalls section, we will explore how a model performs with and without regions of zero signal.

Methods for dealing with class imbalances:

\begin{itemize}
\item
  \href{https://imbalanced-learn.org/stable/user_guide.html\#user-guide}{Scikit-learn's `imbalance-learn' package} (Oversampling, Undersampling and Weighting)
  ``Imbalanced-learn (imported as imblearn) is an open source, MIT-licensed library relying on scikit-learn (imported as sklearn) and provides tools when dealing with classification with imbalanced classes.''
\item
  \href{https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/}{SMOTE} (Oversampling)
  ``a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.''
\item
  \href{https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html}{ADASYN} (Oversampling)
  ``similar to SMOTE but it generates different number of samples depending on an estimate of the local distribution of the class to be oversampled.''
\end{itemize}

\part{Training models with DNA input}\label{part-training-models-with-dna-input}

\chapter{Loss functions, and peak metrics}\label{loss-functions-and-peak-metrics}

When selecting the optimal loss function for your machine learning model in genomics, the decision should be informed by the nature of the problem and the specific type of data you're working with. The principles for choosing loss functions in genomics are similar to those in other machine learning contexts. For regression based tasks, while Mean Squared Error (mse) loss functions have been used, models utilising data and making predictions associated with reads or counts use a Poisson loss function. Another two common loss functions include Binary Cross-Entropy loss and Categorical Cross-Entropy loss, when dealing with classification type predictions.\footnote{\citet{patterson2017}}

\textbf{Mean Squared Error (MSE):}
• Use Case: Regression problems where the goal is to predict continuous values, such as gene expression or coverage levels.
Example: \href{https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1837-6}{DeepImpute}, a deep neural network-based imputation algorithm that allows for accurate imputation of single-cell RNA-seq data. The model is used to estimate missing or low-quality gene expression values in single-cell RNA sequencing datasets. It uses a ``weighted mean squared error (MSE) loss function that gives higher weights to genes with higher expression values. This emphasizes accuracy on high confidence values and avoids over penalizing genes with extremely low values (e.g., zeros)''.\footnote{\citet{cedric2019}}

\textbf{Poisson Loss:}
• Use Case: Count data where the number of events (e.g., read counts in chIP-seq data) follows a Poisson distribution.
Example: A deep learning architecture called \href{https://www.nature.com/articles/s41592-021-01252-x}{Enformer} was used to predict gene expression more accurately in 2021 by integrating long range interactions within the genome. It utilises a \href{https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html}{poisson negative log-likelihood loss function} resulting in a model that was able to integrate information from up to 100 kilobases away.\footnote{\citet{avsec2021}}

\textbf{Cross-Entropy Loss:}
• Use Case: Classification problems where the goal is to predict binary outcomes.
Example: A convolutional neural network was employed to create a software pipeline called \href{https://www.nature.com/articles/s41598-020-64655-4}{CNN-Peaks}, designed to categorically detect ChIP-Seq peaks without relying on traditional peak calling methods or manual inspection. The model utilizes a binary cross-entropy loss function, which was weighted to account for the scarcity of peaks in the data.\footnote{\citet{oh2020}}

\textbf{Categorical Cross-Entropy Loss:}
• Use Case: Multi-class classification problems.
Example: Researchers developed a combined \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10085982/}{model} that integrates cell-free DNA (cfDNA) methylation profile data with ATAC-seq data to enhance cancer detection and tissue-of-origin localization. This approach combines both epigenomic and chromatin accessibility information to improve the accuracy of identifying the specific tissue or organ from which a cancerous signal originates. The model employs a categorical cross-entropy loss function within each component to optimize tissue-of-origin localization, allowing it to effectively determine the most likely source of the cancerous signal.\footnote{\citet{bae2017}}

In the case of this tutorial and running models to predict continuous coverage values from bigwig p-value datasets, I have opted to use a Poisson loss function as the data represents read counts. It is important to remember that ``loss functions can penalize the shapes or the magnitudes (for example, the mean squared error (MSE))''\footnote{\citet{toneyan2022}} when optimising.

\chapter{Training tricks}\label{training-tricks}

\section{Reverse Complements and Sequence Shifts}\label{reverse-complements-and-sequence-shifts}

\textbf{Reverse Complements}

As explained in Part 1, DNA has a double helix structure. When we one-hot encode a segment of DNA in our models using a reference genome, we typically represent only one strand of the double helix. The complement of this strand is the opposite strand, where Adenine (A) pairs with Thymine (T), and Cytosine (C) pairs with Guanine (G). The reverse complement of a DNA strand is obtained by first taking its complement and then reading it in the reverse direction.

This figure shows the reverse complement of a DNA sequence\footnote{\citet{clark2021}}

Training on DNA sequences and augmenting the data with their reverse complements has been shown to improve model accuracy, prediction, and interpretability in DNA sequence-related models. This approach involves ``treating the reverse complement DNA sequence as another sample'' \citep{cao2019}. By incorporating reverse complements, the model is exposed to a wider variety of sequence patterns, which helps reduce overfitting and enhances generalization. As a result, models become better at recognizing patterns regardless of strand orientation. Although the logic to obtain the reverse complement of a DNA strand is straightforward, the Bio.Seq module from the Biopython library provides a simple way to do this. Augmenting your dataset with reverse complements is usually done to training sets, but can be applied to validation and test sets as well.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ Bio.Seq }\ImportTok{import}\NormalTok{ Seq}

\CommentTok{\# Example DNA sequence}
\NormalTok{dna\_sequence }\OperatorTok{=}\NormalTok{ Seq(}\StringTok{"ATGCGTAC"}\NormalTok{)}

\CommentTok{\# Generate the reverse complement}
\NormalTok{reverse\_complement }\OperatorTok{=}\NormalTok{ dna\_sequence.reverse\_complement()}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Original Sequence: "}\NormalTok{, dna\_sequence)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Original Sequence:  ATGCGTAC
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"Reverse Complement: "}\NormalTok{, reverse\_complement)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Reverse Complement:  GTACGCAT
\end{verbatim}

\textbf{Sequence Shifts}

Training on small, random sequence shifts up and downstream by shifting the genomic coordinates of the input sequence is also known as jitter. Jittering adds diversity to the training data by creating slightly different versions of the same sequence. This allows models to be less sensitive to the exact positioning of features, making them more robust to variations in the data. It also allows models to generalise better to unseen data where sites may not always be perfectly aligned. A variation of jittering, called flanking ``extends DNA sequences from its midpoint by X base pairs and takes the left, middle and right input windows of the extended sequence as training samples with the same labels, tripling the size of training set.\footnote{\citet{cao2019}}

Implementing data augmentations using reverse complements and sequence shifts can be approached in different ways. Similar to the `flanking' example, you can either expand your dataset by adding additional augmented data points or apply a random augmentation strategy, where only some data points are randomly augmented while keeping the total number of points in your dataset unchanged. Data augmentations are usually applied only to training sets, however in the context of computer vision ``many research reports have shown the effectiveness of augmenting data at test-time as well''.\footnote{\citet{connor2019}} When implementing augmentations like reverse complements and sequence shifts, these are typically applied after splitting your data into training, validation, and test sets. When applying sequence shifting, it's logical to shift the interval before retrieving the nucleotide sequence from the reference genome. The reverse complement should be applied after retrieving the nucleotide sequence but before one-hot encoding it. If you're using the BioPython library, this works well since BioPython's reverse complement function operates on string inputs.

In the genomics context, a paper on evaluating deep learning for predicting epigenomic profiles used two convolutional neural networks, \href{https://github.com/calico/basenji}{Basenji} and \href{https://github.com/kundajelab/bpnet}{BPNet}, trained on ATAC-seq data, to predict coverage values as a regression. They found that convolutional models trained with augmentations (reverse complement and sequence shifts), ``yielded improved robustness, especially when trained on peak-centered data (BPNet). On the other hand, models that were trained on coverage-threshold data (Basenji) already benefited from the randomly-centered profiles.''\footnote{\citet{toneyan2022}} Additionally, while they initially ``used a MSE and multinomial NLL loss for BPNet, {[}they{]} found that optimization using Poisson NLL yielded better performance.''\footnote{\citet{toneyan2022}} This finding is another motivation of using at poisson loss function in subsequent tutorials.

\section{Hyper-parameter optimisation}\label{hyper-parameter-optimisation}

Which learning rates are commonly used? How many epochs are typically used to train on?

While the learning rate and number of epochs differ by model and study, based on some of the research cited so far, common learning rates are in the range 1e-4\footnote{\citep{cedric2019}, \citep{avsec2021}} to 1e-3\footnote{\citep{cao2019}, \citep{toneyan2022}}. Additionally, some studies apply learning rate decay if the loss function shows no improvement over time\footnote{\citet{toneyan2022}} while others lower the learning rate for fine tuning.\footnote{\citet{avsec2021}}

The number of epochs used to train on differs by quite a margin. In training a convolution neural network to explore the effects of genomic data augmentation, 30 epochs were used.\footnote{\citet{cao2019}} DeepImpute which constructs multiple sub-neural networks for genotype imputation, trains on a maximum of 500 epochs, while the study involving the Basenji and BPNet models were trained on a maximum of 100 epochs. The clear strategy for these larger models involve the use of early stopping if no improvements are evident after 5-10 epochs.

When hyperparameter optimising, the consensus for achieving the best model performance is to train with a high number of epochs to enable the model to confidently learn features as they apply to labels, starting with a high learning rate, and decreasing over time using a learning rate scheduler. Interestingly, a study on binary peak detection using CNNs on ChIP-Seq data manually tuned their model's hyperparameters and found that little changes in performance results\footnote{\citet{oh2020}}. This highlights the challenges of hyperparameter tuning with larger models, where manually fine-tuning is not ideal. How can hyperparamter tuning on these larger models be done in practice?

\textbf{Raytune}

\href{https://docs.ray.io/en/latest/tune/index.html}{Raytune} ``is a Python library for experiment execution and hyperparameter tuning at any scale''. It aids in leveraging state of the art hyperparameter optmisation algorithms while simplifying scaling for larger models. Raytune hyperparameter searching can also be scaled to cloud based clusters without the need for large changes in code structure. Additionally, it supports several machine learning frameworks such as pyTorch and TensorFlow\footnote{\citet{liaw2018}}. One of the strongest current hyperparameter optmisation algorithms is the Asynchronous Successive Halving Algorithm or \textbf{\href{https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/}{ASHA}}. Asha ``exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems''\footnote{\citet{liam2018}}, allowing for faster optimisation and applicability to the larger models common in genomics. A study on predicting the impact of sequence motifs on gene regulation utilised Raytune and the ASHA algorithm to successfully optimise their model's hyperparameters\footnote{\citet{jacob2023}}.

In the genomic context, as a result of complex models using large genomic datasets, hyperparameter tuning using a brute force approach is untenable. Utilising existing libraries such as Raytune and incorporating asynchronous algorithms such as ASHA, has the potential to pave the way forward in improving model performance without unreasonable computational costs.

\chapter{Reproducibility of machine learning models}\label{reproducibility-of-machine-learning-models}

The ability to reproduce machine learning models and results is of extreme importance in the genomic context. According to an article on reproducibility standards for machine learning in the life sciences, the bronze standard for reproducibility includes access to the original data, trained model and source code. The silver standard incorporates computational environment considerations as well as inherent non-determinism, while the gold standard requires automation such that the experiment is reproducible with a single command.\footnote{\citet{ben2021}}

\textbf{Seeding}

Seeding runs and setting random number seeds aids in eliminating the inherent non-determinism of creating data splits and training models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Seeding to ensure reproducibility}
\ImportTok{import}\NormalTok{ random}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ torch}

\NormalTok{seed }\OperatorTok{=} \DecValTok{42}  \CommentTok{\# or any other integer}
\NormalTok{random.seed(seed)}
\NormalTok{np.random.seed(seed)}
\NormalTok{torch.manual\_seed(seed) }\CommentTok{\#CPU}
\NormalTok{torch.cuda.manual\_seed(seed) }\CommentTok{\#GPU}
\NormalTok{torch.backends.cudnn.deterministic }\OperatorTok{=} \VariableTok{True}  \CommentTok{\# To ensure deterministic behavior}
\NormalTok{torch.backends.cudnn.benchmark }\OperatorTok{=} \VariableTok{False}     \CommentTok{\# Ensures reproducibility}
\end{Highlighting}
\end{Shaded}

This seeding example shows a seed being used for python's built in random library, numpy's random library, and the PyTorch library for both CPUs and GPUs. Additionally, PyTorch's CuDNN library accelerates computations on GPUs. By setting CuDNN to deterministic mode and disabling benchmarking, you ensure that the same algorithm is used in each run. This approach reduces variability due to hardware differences at the cost of computational efficiency.

\textbf{Dashboarding}

Dashboarding can be used to keep track of model performance as well as model and training parameters by visualising logged data. Dashboards such as Weights and Biases (WandB) can be leveraged to keep track of model runs, hyperparameters and even trained models and data through uploading them as artifacts.

\includegraphics{images/wandb.png}

Source: \href{https://docs.wandb.ai/quickstart}{WandB}

Real time updates on metrics are important in the genomic context when models can take extended periods of time to train. Furthermore, downloading and sharing specific models, additional artifacts and training logs with information on environment settings becomes easier when utilising dashboards. While not explicitly ensuring or enabling reproducibility, dashboarding helps provide transparency.

\chapter{Testing}\label{testing}

Testing and evaluating genomic models is similar to other machine learning contexts in that it involves assessing model performance using metrics such as accuracy, precision, recall, and F1-scores. However, the genomic context often requires additional considerations due to the complexity and high-dimensionality of genomic data. There are several pitfalls which when fallen into, inflate model performance and make test metrics untrustworthy. Genomic data can be extremely imbalanced, as we will find out in subsequent tutorials. In these unbalanced classes scenarios, the choice of test metric usually between the area under the receiver operating characteristic (auROC) and the area under the precision-recall curve (auPRC), as only predicting the majority class will result in high accuracy for categorical models.

\includegraphics{images/auroc_auprc.png}

Source: \href{https://juandelacalle.medium.com/how-and-why-i-switched-from-the-roc-curve-to-the-precision-recall-curve-to-analyze-my-imbalanced-6171da91c6b8}{BlogPost}

In genomic situations with extreme imbalance in data, plotting ROC and precision-recall curves and calculating the auROC and auPRC is beneficial to properly test your model's performance. AuROC and auPRC both provide insight into the model's ability to distinguish between classes and are both functions of recall (true positive rate). ``In many genomics problems, high recall can be achieved at a very low {[}false positive rate{]} owing to the large number of negatives in the test set, making it easy to obtain a high auROC even when false positives vastly outnumber true positives''\footnote{\citet{whalen2022}}. However, auPRC offers a more focused evaluation as the absolute number of false positives is taken into account in precision versus the false positive rate. Therefore, a model trained on imbalanced data that predicts many false positives could have a high auROC and a low auPRC. In cases where we are interested in a minority of positive classes, research suggests using the auPRC as a measure of performance\footnote{\citet{whalen2022}}.

In regression settings, classic metrics such as the mean squared error, and mean absolute error are used alongside measures of variance (\(R^2\)) and correlation (pearson correlation). It is important to note that differences in the order of magnitude of multiple sets of predictions can affect MSE metrics when evaluated together. For example, if a multi-task model has two prediction sets where one set ``has values entirely between 1,000 and 10,000 and the other {[}set{]} has values between 0.01 and 0.1, then evaluating the model simply using mean squared error (MSE) across the two tasks will largely ignore the second task.''\footnote{\citet{whalen2022}}

In the genomic context one can also perform marginalisation experiments to ensure models are recognizing biologically relevant patterns. ``Marginalisation is a method that requires summing over the possible values of one variable to determine the marginal contribution of another''\footnote{\citet{jonny2018}}. Given a model trained on DNA sequences, performing a marginalisation experiment using sequence motifs is possible. ``Motifs are short, recurring patterns in DNA that are presumed to have a biological function''\footnote{\citet{pat2006}}. Marginalising with motifs involves comparing a model's predictions before and after the motif is inserted into a test/prediction set. The difference in predictions allows us to quantify the influence of a motif on a model's predictions and understand how well our model recognises biological patterns. More specific types of marginalisation experiments include variant effect prediction, which aims to understand the effect of specific genetic variants in sequences on a model's predictions.

\chapter{Software libraries for model building}\label{software-libraries-for-model-building}

\section{gReLU}\label{grelu}

gReLU is a Python library to train, interpret, and apply deep learning models to DNA sequences''. The gRelu library contains a model zoo allowing for easy access to several models such as Borzoi, Enformer, or a dilated convolutional model based on the BPnet model architecture. Some of these models can be imported pre-trained. Additionally, simpler base models and convolutional neural networks are also available. On top of access to already built models, the software library allows for designing your own models.

\includegraphics{images/grelu.png}

Source: \href{https://github.com/Genentech/gReLU/blob/main/README.md}{gRelu}

Their documentation contains all their available functions and includes tutorials on using gRelu.

\section{Kipoi}\label{kipoi}

Kipoi is a repository of ready-to-use trained models for genomics. Referring back to the reproducibility of machine learning models, Kipoi contains 2206 different gold standard models, available to be downloaded and tested in a few lines of code.

\includegraphics{images/kipoi.png}

Source: \href{https://kipoi.org/docs/}{Kipoi}

Similarly to gRelu, they include several use case tutorials and a model zoo. Downloaded models can also be built upon to conduct further research as links to the github source code of each model are provided.

\part{ML pitfalls in genomics}\label{part-ml-pitfalls-in-genomics}

\chapter{Pitfalls overview}\label{pitfalls-overview}

Applying machine learning in the field of genomics comes with its own challenges. Genomic data is highly complex, featuring high dimensionality, heterogeneity, and noise. It is important to consider the assumptions models make and whether those assumptions hold true within the data. According to research, available \href{https://www.nature.com/articles/s41576-021-00434-9}{here}, on pitfalls in machine learning, the most common errors include not taking into account distributional differences between contexts or batches, dependencies within the data, confounding effects distorting true relationships, unbalanced classes and leaking information between datasets.\footnote{\citet{whalen2022}} Lets examine the pitfalls mentioned in the research above and methods to mitigate these effects in more detail.

\includegraphics{images/pitfalls.png}

Source: \href{https://www.nature.com/articles/s41576-021-00434-9/figures/1}{Nature Reviews Genetics}

\textbf{Distributional differences}

Distributional differences between the context in which a model was trained and tested, and the context in which a model makes predictions, results in different model performances. These different contexts can be attributed to batch effects or applying the model on different cell types. ``One should expect that performance will be higher in {[}the same setting{]} than on {[}different settings{]}\footnote{\citet{whalen2022}}. It is crucial then, for models which aim to predict biological relationships across contexts to be validated and tested across these different settings that accurately reflect the real-world variability. Even models that aim to predict across one context such as cell type-specific models common in disease prediction, are still susceptible to this pitfall. When using data from multiple experiments, batch effects can introduce variability that lead to differences between training, test, and prediction sets. To address this, it is important to consider batch effects during model development and evaluation. One approach is to ensure that training and test sets are sourced from different batches to evaluate the generalisability of the model and prevent inflated performance. Additionally, applying batch effect correction techniques such can help account for these distributional differences.

\textbf{Dependent examples}

When studying machine learning, a constant assumption is that of independence. Whether it be independent test sets or data points being independent of each other, the assumption of independence is key in creating fair models. Many biological processes in genomics are not independent.\footnote{\citet{allis2015}} A research paper focused on understanding the pitfalls of neural networks predicting across cell types found that several models had been evaluated on test sets comprising of cell type independent of training, but not independent of chromosomes leading to inflated performances.\footnote{\citet{jacob2020}} That is, gene expression predictions were evaluated on different cells but on the same chromosomes. The inflated performances were due to chromosomes ``themselves {[}being{]} dependent across samples because the underlying functional activity is generally shared''\footnote{\citet{whalen2022}}. While hard to identify in the designing phase even through visualisation, mitigation techniques include preventing overfitting during evaluation and group k-fold cross-validation to prevent leakage between training and test sets.\footnote{\citet{whalen2022}}

\textbf{Confounding}

Confounders are additional variables that affect the variables being studied, resulting in models not capturing the correct relationships in the data.\footnote{\citet{mo2012}} ``Confounding in genetic studies can arise from unmodelled environmental factors and population structure, as well as other factors''.\footnote{\citet{whalen2022}} In the context of ATAC-Seq and chIP-Seq data, differences in the sequencing depth of the experiment can have a confounding effect on models if not handled. As explained in part 1, ATAC-Seq and chIP-Seq data consists of reads that are alligned to a reference genome and aggregated. The sequencing depth of the experiment refers to on average how many times a region was sequenced.\footnote{\citet{sims2014}} A higher sequencing depth means a higher overall coverage level. If not accounted for, models using data of different sequencing depths are confounded. Similarly when predicting on a dataset with a different sequencing depth than the training set, the model would biased. While sequencing depth as a confounder can be more easily corrected by downsampling the raw reads from the dataset with higher sequencing depth, other confounding effects can be harder to account for. However, it is possible to use statistical tools such as PEER\footnote{\citet{oliver2012}}, Inter-sample Correlation Emended (ICE) and Surrogate Variable Analysis (SVA)\footnote{\citet{jen2010}} to understand confounders in your dataset.

\textbf{Leaky pre-processing}

Leaky pre-processing involves information leaks between the training and test sets resulting in altered testing metrics. In genomics, as a result of many dependencies, pre-processing steps involving the dataset as a whole can introduce this bias. This includes standardisation techniques, supervised feature selection or principle component analysis, before splitting data into test splits.\footnote{\citet{whalen2022}} A solution to this is to perform these transformations and analysis after data splits, preferably within cross-validation. Leaky preprocessing has been prevalent in various genomic fields, including microarray analysis, DNA methylation, gene expression studies, and more, often leading to misleading results.\footnote{\citet{whalen2022}}

\textbf{Unbalanced classes}

Unbalanced datasets can be common in genomics often necessitating the use of statistical methods to validate the few positive instances amid vast amounts of data. In these cases, the pitfall is a model ``learns most of the target concepts of the majority class and learns target concepts from the minority class poorly or not at all.''\footnote{\citet{muk2014}} This is particularly evident in disease related tasks where the focus is on a few disease causing genes or non-coding variants.\footnote{\citet{muk2014}} Additionally, many experiments that implement conservative significance thresholds to determine true signals involve data imbalance, such as peak detection\footnote{\citet{oh2020}}, gene expression\footnote{\citet{avsec2021}}, and chromatin accessibility. The extensive size of the human genome exacerbates this issue when the areas of interest are small. Researchers address this imbalance by employing balancing algorithms that oversample the negative class and undersample the majority class. For instance, in training models to predict functional peaks from ChIP-seq or chromatin accessibility data, an approach might involve using all identified peaks along with a matching number of negative regions, thus effectively undersampling the majority class. For datasets with no such negative regions, researchers have to construct their own. While such imbalances are commonly discussed in classification, they also pose challenges in regression models predicting quantitative outcomes, where performance may be compromised in regions with sparse data, such as genomic areas or genes with low read counts in single-cell genomics studies.\footnote{\citet{whalen2022}}

\part{Using existing models}\label{part-using-existing-models}

\chapter{Using gReLU models}\label{using-grelu-models}

Continuing from previous tutorials, the next tutorial similarly uses chIP-Seq data from Encode Experiment \href{https://www.encodeproject.org/experiments/ENCSR817LUF/}{ENCSR817LUF} trained on a gRelu model in order to to predict the coverage levels and examine several of the pitfalls mentioned in section 4. An additional tutorial is included to explain how data was pre-processed for gRelu to explore

Tutorial 1.5: Preprocessing Data for gRelu Models (interactive)

Tutorial 1.5: Preprocessing Data for gRelu Models (nbviewer)

Tutorial 2: Training Models with gRelu and Examining Pitfalls (interactive)

Tutorial 2: Training Models with gRelu and Examining Pitfalls (nbviewer)

  \bibliography{book.bib,packages.bib}

\end{document}
